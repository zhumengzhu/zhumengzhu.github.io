[{"content":"背景 最近在做一个 APP 的后端开发，为了方便和客户端同学联调，用了一台阿里云的机器来部署服务，本文简单记录相关部署和调试流程，以备查阅。\n服务环境准备 考虑到测试环境对性能和稳定性要求不高，为了节省成本，选择之间将 MySQL 和 Redis 都安装在同一台机器上，其中 Linux 版本为 CentOS Linux release 7.9。\n配置 SSH 密钥登录 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 # 1. 客户端生成密钥对，系统有提示，一路回车即可 ssh-keygen -t ed25519 -f ~/.ssh/id_ed25519_aliyun -C \u0026#34;your_email@example.com\u0026#34; # 2. 将私钥添加到 ssh-agent eval (ssh-agent -c) # 启动 ssh-agent (fish 写法) ssh-add ~/.ssh/id_ed25519_aliyun ssh-add -l # 查看已添加的公钥 # 3. 将公钥添加到远程服务器 cat ~/.ssh/id_ed25519_aliyun.pub | ssh username@server_ip \u0026#34;cat \u0026gt;\u0026gt; ~/.ssh/authorized_keys\u0026#34; # 4. 配置 fish 自动启动 ssh-agent # 在 ~/.config/fish/config.fish 中添加 if status is-interactive # 检查 ssh-agent 是否已运行 if not test -e $SSH_AUTH_SOCK eval (ssh-agent -c) set -Ux SSH_AUTH_SOCK $SSH_AUTH_SOCK set -Ux SSH_AGENT_PID $SSH_AGENT_PID end # 添加密钥（如果未添加） ssh-add -l \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 if test $status -eq 1 ssh-add ~/.ssh/id_ed25519_aliyun ssh-add ~/.ssh/id_ed25519 end end # 5. 编辑 SSH 配置文件 # 在 ~/.ssh/config 添加主机配置(之后直接 ssh aliyun_test_server 就可以登录服务器了) Host aliyun_test_server HostName server_ip User root UseKeychain yes AddKeysToAgent yes PreferredAuthentications publickey IdentityFile ~/.ssh/id_ed25519_aliyun 安装 Java、MySQL、Redis 等 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 1. Java（如果安装的是 java-1.8.0-openjdk，则仅仅是 JRE，不包含额外的开发工具（如 javac、jps、jstack 等） sudo yum install java-1.8.0-openjdk-devel # 2. Redis # 安装 sudo yum install -y redis # 启动 Redis systemctl start redis # 设置开机启动 systemctl enable redis # 验证 Redis redis-cli ping # 3. MySQL sudo yum install -y mysql-community-server # 启动 MySQL 服务 sudo systemctl start mysqld # 设置开机启动 sudo systemctl enable mysqld # 查看默认的 root 用户密码 grep \u0026#39;temporary password\u0026#39; /var/log/mysqld.log # 登录 MySQL mysql -u root -p 部署 Java 服务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 1. 本地构建得到 jar 包 mvn clean package 2. 打包本地 lib 包（其中包含所有 maven 依赖 jar） # 考虑使用 maven-assembly-plugin 插件生成包含所有依赖的 JAR 文件，省略该步骤 zip -r lib.zip lib/ 2. 使用 scp 传输到远程服务器 scp target/your-project-jar-with-dependencies.jar user@remote-server:/path/to/deploy scp target/lib.zip user@remote-server:/path/to/deploy unzip lib.zip -d lib/ # 登录远程服务器后解压 3. 编写启动脚本启动服务 # vim restart.sh #!/bin/bash echo \u0026#34;************ 查找进程 **************\u0026#34; str=`ps aux | grep \u0026#34;biz-1.0.0.jar\u0026#34; | grep -v grep | awk \u0026#39;{print $2}\u0026#39;` echo $str biz kill -9 $str if [ \u0026#34;$?\u0026#34; -eq 0 ]; then echo \u0026#34;kill success\u0026#34; else echo \u0026#34;kill failed\u0026#34; fi echo \u0026#34;************ 杀掉进程 **************\u0026#34; export SUPERVISOR_LOG_HOME=\u0026#34;logs\u0026#34; JAVA_OPTS=\u0026#34;-server -Xmx512m -Xms512m -Xmn256m -Xss256k -XX:MetaspaceSize=96m\u0026#34; JAVA_OPTS=\u0026#34;$JAVA_OPTS -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=localhost:5005\u0026#34; JAVA_OPTS=\u0026#34;$JAVA_OPTS -XX:+DisableExplicitGC\u0026#34; JAVA_OPTS=\u0026#34;$JAVA_OPTS -XX:+UseConcMarkSweepGC\u0026#34; JAVA_OPTS=\u0026#34;$JAVA_OPTS -XX:CMSMaxAbortablePrecleanTime=5000\u0026#34; JAVA_OPTS=\u0026#34;$JAVA_OPTS -XX:+CMSClassUnloadingEnabled \u0026#34; JAVA_OPTS=\u0026#34;$JAVA_OPTS -XX:+CMSParallelRemarkEnabled\u0026#34; JAVA_OPTS=\u0026#34;$JAVA_OPTS -XX:+UseFastAccessorMethods\u0026#34; JAVA_OPTS=\u0026#34;$JAVA_OPTS -XX:+UseCMSInitiatingOccupancyOnly\u0026#34; JAVA_OPTS=\u0026#34;$JAVA_OPTS -XX:+ExplicitGCInvokesConcurrent\u0026#34; JAVA_OPTS=\u0026#34;$JAVA_OPTS -XX:CMSInitiatingOccupancyFraction=80\u0026#34; JAVA_OPTS=\u0026#34;$JAVA_OPTS -XX:SurvivorRatio=8\u0026#34; JAVA_OPTS=\u0026#34;$JAVA_OPTS -XX:+HeapDumpOnOutOfMemoryError -XX:+OmitStackTraceInFastThrow -XX:HeapDumpPath=$SUPERVISOR_LOG_HOME/java.hprof\u0026#34; JAVA_OPTS=\u0026#34;$JAVA_OPTS -Xloggc:$SUPERVISOR_LOG_HOME/gc.log -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps\u0026#34; JAVA_OPTS=\u0026#34;$JAVA_OPTS -Dlog.path=$SUPERVISOR_LOG_HOME\u0026#34; nohup java $JAVA_OPTS -Dspring.profiles.active=test -jar biz-1.0.0.jar 2\u0026gt;\u0026amp;1 \u0026amp; echo \u0026#34;************ 启动成功 **************\u0026#34; 目前这个非自动化部署的流程，操作起来还是比较繁琐。\n使用 SSH 隧道进行远程调试 出于安全考虑，debug 绑定只绑定本地回环接口，然后通过 SSH 隧道进行连接。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 # 1. 在远程服务器上启动 Java 应用时添加调试参数 # 配置 Java 应用只监听 localhost java -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=localhost:5005 YourApp # 2. 建立 SSH 隧道 # 语法：ssh -L 本地端口:远程主机:远程端口 用户名@远程主机 # 这里我实际用的是配置在 .ssh/config 中的主键昵称-aliyun_test_server ssh -L 5005:localhost:5005 aliyun_test_server # 如果需要在后台运行 ssh -fNL 5005:localhost:5005 aliyun_test_server # 3.配置 IDE - 配置远程调试 - 连接地址：localhost:5005（使用本地转发的端口 # 4. 常用的 SSH 隧道命令选项 # -f: 后台运行 # -N: 不执行远程命令 # -L: 本地端口转发 # -v: 显示详细信息（调试用） # 详细模式（便于调试） ssh -vNL 5005:localhost:5005 root@server_ip # 后台运行模式 ssh -fNL 5005:localhost:5005 root@server_ip # 指定密钥文件 ssh -i ~/.ssh/id_ed25519_aliyun.pub -fNL 5005:localhost:5005 root@server_ip # 5. 检查隧道状态 # 查看已建立的连接 netstat -tunlp | grep 5005 # 查看 SSH 进程 ps aux | grep ssh # 测试端口转发是否成功 nc -zv localhost 5005 # 6. 关闭 SSH 隧道 # 查找 SSH 进程 ps aux | grep ssh # 关闭特定的 SSH 隧道 kill \u0026lt;进程ID\u0026gt; # 或者关闭所有 SSH 隧道（谨慎使用） pkill -f \u0026#34;ssh -fNL\u0026#34; 遇到的问题 一个负责登录认证的拦截器未注册 比较奇怪的点在于，本地这个拦截器是正常的，但部署到远程之后它就失效了(远程 Debug 类 HandlerExecutionChain#applyPreHandle发现确实没有这个拦截器)，导致一些需要登录认证的接口无法使用。\n代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Configuration public class InterceptorConfig extends WebMvcConfigurationSupport { @Bean public JwtInterceptor authenticationInterceptor() { return new JwtInterceptor(); } @Override public void addInterceptors(InterceptorRegistry registry) { registry.addInterceptor(authenticationInterceptor()) .addPathPatterns(\u0026#34;/**\u0026#34;); } } 目前怀疑可能是某些类冲突导致的（本地类加载顺序和远程不一样），具体原因还在排查(截至05.10)。\n","date":"2025-05-10T17:13:00+08:00","permalink":"https://zhumengzhu.github.io/2025/05/aliyun-deployment-debugging-java-services/","title":"阿里云部署调试 Java 服务记录"},{"content":"要点简介：\n服务规模：40台4C8G机器的生产级模型推理集群 业务基准：稳定支撑1万QPS，服务SLA要求50ms@TP9999 问题特征：无代码变更的情况下出现性能断崖 引发思考：看似稳定的系统为何会突然性能劣化？ Previous Next \u0026nbsp; \u0026nbsp; / [pdf] View the PDF file here. ","date":"2024-07-19T11:30:00+08:00","permalink":"https://zhumengzhu.github.io/2024/07/in-depth-jvm-performance-tuning-guide/","title":"JVM性能调优实战分享"},{"content":"背景 在对某抽奖压测服务时(dev\u0026amp;test环境)，发现当 QPS 达到 100 左右时，CPU 使用率就达到 100%，对应的 on-CPU 火焰图显示，有接近 56% 的 CPU 时间在进行 Deoptimization： 经调研，怀疑是该 JDK Bug 导致：https://bugs.openjdk.org/browse/JDK-8243615 类似的反馈还有：\nhttps://mail.openjdk.org/pipermail/hotspot-dev/2016-June/023316.html https://mail.openjdk.org/pipermail/hotspot-compiler-dev/2016-June/023278.html https://github.com/trinodb/trino/issues/6405 https://trino.io/blog/2021/10/06/jvm-issues-at-comcast.html#jit-recompilation 复现方法 测试代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 public class UnstableIfTest { static int compute(double x) { return (int) (Math.sin(x) + Math.cos(x)); } static void hotMethod(int iteration) { if (iteration \u0026lt; 20) { compute(78.3); } else if (iteration \u0026lt; 40) { compute(78.3); } else if (iteration \u0026lt; 60) { compute(78.3); } else if (iteration \u0026lt; 80) { compute(78.3); } else if (iteration \u0026lt; 100) { compute(78.3); } else { compute(78.3); } } static void hotMethodWrapper(int iteration) { int count = 100_000; for (int i = 0; i \u0026lt; count; i++) { hotMethod(iteration); } } public static void main(String[] args) { for (int k = 0; k \u0026lt; 1000; k++) { long start = System.nanoTime(); hotMethodWrapper(k + 1); System.out.println(\u0026#34;iteration \u0026#34; + k + \u0026#34;: \u0026#34; + (System.nanoTime() - start) / 1_000_000 + \u0026#34;ms\u0026#34;); } } } 执行步骤：\n1 2 3 4 5 6 7 8 # Step1： 编译代码 javac UnstableIfTest.java # Step2：执行代码 java -XX:PerMethodRecompilationCutoff=4 UnstableIfTest # Step3：在代码执行期间，使用 arthas 或者 async-profiler 收集火焰图 profiler start -e cpu -d 120 结果： 试分析原因 实验 注： 这里需要使用 java21，因为之前的版本还不支持打印 deoptimization 日志。\n1 2 3 4 5 6 7 8 9 # 使用 java21 执行一下命令 java -XX:+UnlockDiagnosticVMOptions -XX:PerMethodRecompilationCutoff=4 -Xlog:deoptimization=debug UnstableIfTest \u0026gt; log.txt # 大量 deoptimization 日志 [18.896s][debug][deoptimization] cid= 632 level=4 UnstableIfTest.hotMethod(I)V trap_bci=67 unstable_if none pc=0x0000000113084938 relative_pc=0x0000000000000138 [18.896s][debug][deoptimization] cid= 632 level=4 UnstableIfTest.hotMethod(I)V trap_bci=67 unstable_if none pc=0x0000000113084938 relative_pc=0x0000000000000138 [18.896s][debug][deoptimization] cid= 632 level=4 UnstableIfTest.hotMethod(I)V trap_bci=67 unstable_if none pc=0x0000000113084938 relative_pc=0x0000000000000138 [18.896s][debug][deoptimization] cid= 632 level=4 UnstableIfTest.hotMethod(I)V trap_bci=67 unstable_if none pc=0x0000000113084938 relative_pc=0x0000000000000138 .... 第 98 次迭代耗时只有 3ms： 第 99 次迭代，耗时 631ms： （注意在第 98 次迭代时，cid=621 osr，到第 99 次迭代时，cid=624） 在两次迭代直接，总共打印了 10万次(102086-2086=100000) deoptimization 日志，这正好是方法 hotMethodWrapper 内部循环的次数：\n1 2 3 4 5 6 7 static void hotMethodWrapper(int iteration) { int count = 100_000; for (int i = 0; i \u0026lt; count; i++) { hotMethod(iteration); } } ","date":"2023-09-23T14:14:52+08:00","permalink":"https://zhumengzhu.github.io/2023/09/root-cause-jvm-deoptimization-storm/","title":"JIT deoptimization 风暴导致 CPU 使用率飙升"},{"content":"背景 staging 环境，某个车联网服务内存出现了内存泄露，相关同学在 06-14 及 06-15 两天分别采集了两个 dump 文件，以下简单记录分析过程。\n内存泄露最常用的排查工具是 Eclipse Memory Analyzer (MAT)，使用 MAT 打开两个 dump 文件。\n首先看 Leak Suspects 疑点一：JarFile org.springframework.boot.loader.jar.JarFile 是 Spring Boot 的类加载器，用于加载 jar 包。\n猜测此问题与 Druid issue-3808 类似：\n所有 jar 包被加载的次数几乎相同； 内存 dump 中相关对象的数量只有几千个，和请求量无关，更像是某些定时任务在做类加载： 06-14 的 dump 中加载次数约为 3250； 06-15 的 dump 中加载次数约为 1260； 可以通过升级或降级 mysql-connector-java 版本进行验证。\n疑点二：LockPubSub org.redisson.pubsub.LockPubSub 是 Redisson 的分布式锁实现，用于分布式锁的订阅和消息通知：加锁对象如果已经被锁定需要等待的时候，通过 pubsub 订阅消息注册一个 listener 来等待锁资源。\n分析 06-14 的内存 dump，发现 key 7e5axxxc00:gateway:xx:zz:msg:task:lock:c53fxxxaa13a 的 listeners 达到 33206 个，内存占用达到 1.32 GB。\n此问题与 redisson-issues-3577有些相似，但我在日志中没有找到这个服务锁的 timeout 日志（可能因为业务代码没打印），而只有获取锁失败的日志。\n另外，业务日志中只有释放锁的日志，而没有加锁的日志(也可能因为业务代码没打印)。 进一步分析 1、对内存泄露部分的 char 数组进行聚合的结果（2.12 GB 的内存泄露，日志信息 1.66GB，占比接近 80%）： 2、其中一个 char 数组到 gc roots 的路径： 这也是内存泄露的路径：OpenTelemetry 在做链路追踪时，会将一次 gRPC、HTTP 调用或 MQ 消费过程中的 Metrics 以及日志信息都放到它的 Context 对象中，这次调用或消费不结束，这些内存就会被引用而释放；业务使用分布式锁的方式可能存在问题，导致很多请求无法结束，大量对象存活无法释放。\n3、以下为该服务打印日志的频率监控\u0026mdash;-每分钟几千条日志。 4、另外通过Kibana 查询发现，有很多动辄几十 KB 的业务日志（敏感数据就不贴图了）。 5、服务的 GC 信息 综合上述信息，可以确定服务内存泄露，高概率是大日志导致的。\n解决方法 升级或降级 mysql-connector-java 版本 避免大日志 参考资料 Redis分布式锁-这一篇全了解(Redission实现分布式锁完美方案) https://github.com/redisson/redisson/issues/4216 https://github.com/redisson/redisson/issues/4294 ","date":"2023-07-17T11:46:30+08:00","permalink":"https://zhumengzhu.github.io/2023/07/memory-leak-issue-in-a-service/","title":"服务内存泄露问题排查记录"},{"content":"Q1、JDK 和 JRE 的区别 答：JDK 是 JRE 的父集，我们 POD 中需要安装 JDK，以便能运行一些诊断工具。\nJDK(Java Development Kit，Java 开发工具包) ，是整个JAVA的核心，包括了Java运行环境JRE（Java Runtime Envirnment），一堆Java工具（javac/java/jdb等）和Java基础的类库），包含JVM标准实现及Java核心类库。 JRE(Java Runtime Environment Java 运行环境) ，是 JDK 的子集，也就是包括 JRE 所有内容，以及开发应用程序所需的编译器和调试器等工具。 见：https://docs.oracle.com/javase/8/docs/ Q2、Oracle JDK 究竟从哪个版本开始收费？ 答：jdk8u201 和 jdk8u202 是 orcale jdk8 最后的免费版本，从 jdk8u211 和 jdk8u212 开始收费。参考：https://www.cnblogs.com/xuruiming/p/12881503.html\n如果使用 Orcale JDK，建议 jdk8u201: Q3、如何下载 Orcale JDK 8u201? 答：从 Oracle 官网下载：https://www.oracle.com/java/technologies/javase/javase8-archive-downloads.html\nQ4、Oracle JDK8u 和 OpenJDK8u 有什么区别？ 答：最主要的是授权协议不同。Orcale 的高版本商用时收费，OpenJDK 则完全免费。\n其次还有OpenJDK源代码不完整、部分源代码用开源代码替换、OpenJDK只包含最精简的JDK等问题，但一般来说，这些影响可以忽略。\nQ5、JDK8 的支持到什么时候结束？ 答：2030年12月31日，https://endoflife.date/java Q6、OpenJDK 支持 Java Flight Recorder 吗？ 答：支持，见：低延迟 Profile 工具 Flight Recorder 被移植到 Java 8。\n开启方法：\n1 -XX:+UnlockCommercialFeatures -XX:+FlightRecorder Q7、OpenJDK8 中的 Java Flight Recorder 是商用特性吗？ 答：不是，该特性在 OpenJDK 是完全免费的，见：Get started with JDK Flight Recorder in OpenJDK 8u。\n但要注意：Oracle 的 JDK11 之前的版本中 JFR 是商用特性，所以需要加 -XX:+UnlockCommercialFeatures 开启；同时为了兼容性，使用 OpenJDK 也要添加该参数。\nQ8、OpenJDK8 支持识别 Docker 容器的资源限制吗？ 答：取决于版本。\n比如：从 jdk8u191 开始完整支持，不需要需要使用 -XX:ParallelGCThreads=4 -XX:ConcGCThreads=1 -XX:G1ConcRefinementThreads=5 -XX:ActiveProcessorCount=8 这些参数指定 GC 线程数。\n详见：JVM 对容器化支持的参数（强烈建议，参考该文章在 docker 中进行验证）。\n小于 jdk8u131 版本不支持； 大于等于 jdk8u131，小于 jdk8u191 实验性支持； 大于等于 jdk8u191 开始完整的支持； Q9、如何下载 OpenJDK8？ 答： 很多，这里给出两个：Azul Zulu Downloads，OpenLogic OpenJDK Downloads 。\n特别地，如果是在个人开发电脑，则强烈建议使用 SDKMAN ，它支持并行版本管理，Java 生态的各种软件包，包括 JDK、、Kotlin、Groovy、Scala、Maven、Gradle 等等。\n","date":"2023-07-11T12:35:11+08:00","permalink":"https://zhumengzhu.github.io/2023/07/oracle-jdk-vs-openjdk-selection-guide/","title":"Oracle JDK 和 OpenJDK 如何选择？"},{"content":"背景 某 Java 服务的老年代内存占用持续高位，怀疑可能存在内存泄漏的风险。 排查记录 分析堆内存 dump dump\u0026amp;\u0026amp;分析 jmap -dump:[live],format=b,file=\u0026lt;file-path\u0026gt; \u0026lt;pid\u0026gt; live: if set, it only prints objects which have active references and discards the ones that are ready to be garbage collected. This parameter is optional. format=b: specifies that the dump file will be in binary format. If not set, the result is the same. file: the file where the dump will be written to pid: id of the Java process 1、不进行 FullGC 直接 dump 内存\n1 2 3 jmap -dump:format=b,file=dump.hprof 1 tar -cvzf dump.zip dump.hprof # 压缩，以方便下载 Leak Suspects 显示主要疑点全部与 com.ecwid.consul.v1.Response 有关。 2、强制 FullGC 然后 dump 内存 依次执行以下命令，获取内存 dump 然后进行压缩（提高文件下载速度），然后通过服务治理平台-文件下载功能下载到本地，使用 eclipse mat 或者 VisualVM 进行分析。\n1 2 3 jmap -dump:live,format=b,file=dump.hprof 1 tar -cvzf dump.zip dump.hprof 结论 Old Gen 虽然会达到很高占比，但总是能最终下降到低位，说明不存在严格意义上的内存泄露； 根据 Leak Suspects 报告，Old Gen 占用高主要是类 com.ecwid.consul.v1.Response 导致，它代表从 consul 拉取的数据，包括两类； CustomConfigWatch（javakit）获取的数据大小约 2MB； ConsulCatalogWatch（SpringBoot）获取的数据大小约 170KB； 这些数据会导致产生 Humongous 对象：比如网络层面接收这些数据时需要分配大的 byte 数组，转为 HttpResponse 时分配 char 数组（CharArrayBuffer）； 根据 Dominator Tree，除了 consul 数据，监控数据占内存同样很高 分析 GC 日志 结论 Old Gen 占比高并不是因为长生命周期对象晋升，而是大量分配 Humongous 对象（简称 H-Obj）\u0026mdash;-因为根据监控，在没有 GC 时，Old Gen 的却一直在增长； 每当进行 H-obj 分配时，就会触发 Mixed GC 的并发标记循环，进而导致一次 YGC（inital mark），回收死亡的 H-Obj；但如果当前处于 Mixed GC 阶段，则不会再触发一次； 由于 H-obj 的分配频率非常高，因此实际会一直处于 Mixed GC 阶段（中间可以夹杂多次 YGC）； 在 YGC 以及 Mixed GC 的 cleanup 阶段，Old Gen 会大幅下降，因为此阶段会对 H-Obj 对象进行清理； 虽然强制指定 -Xmn1g，但实际 Young 区还是会缩小； dev \u0026amp; test 环境，在出现如下 GC 日志时，Old Gen 只有小幅下降： 1 2 3 4 5 6 - 2023-06-30T07:10:32.869+0000: 3600.316: [GC pause (G1 Humongous Allocation) (mixed) 3600.316: [G1Ergonomics (CSet Construction) start choosing CSet, _pending_cards: 26151, predicted base time: 42.80 ms, remaining time: 157.20 ms, target pause time: 200.00 ms] - 3600.316: [G1Ergonomics (CSet Construction) add young regions to CSet, eden: 384 regions, survivors: 0 regions, predicted young region time: 3.51 ms] - 3600.316: [G1Ergonomics (CSet Construction) finish adding old regions to CSet, reason: predicted time is too high, predicted time: 1.00 ms, remaining time: 0.60 ms, old: 153 regions, min: 83 regions] - 3600.316: [G1Ergonomics (CSet Construction) finish choosing CSet, eden: 384 regions, survivors: 0 regions, old: 153 regions, predicted pause time: 199.40 ms, target pause time: 200.00 ms] - G1 预测回收 Old Gen 时只剩余 0.6 ms，因此进行回收时只选取了少量的 Old Gen，且没有回收 Humongous 对象； - 根据 jdk8u 的源码，此预测似乎基于对先前若干次回收时间的统计； 整体结论 Old Gen 的快速上升主要是由于大量的 H-obj 分配，大对象的来源按频率排序，目前主要有 3 个: ConsulCatalogWatch（SpringBoot）获取的数据大小约 170KB，约每 2 秒一次； CustomConfigWatch（javakit）获取的数据大小约 2MB，约每 27 秒一次； Prometheus\u0026amp;Metrics 监控数据，小于等于 1.3 MB，约 1 分钟一次？？ H-obj 的分配速率与年轻代对象的增长速率大约成正比，且往往小于年轻代的增长速率； 但存在一些特殊情况，在 mixed 回收阶段，由于 Old Gen 中可回收的 region 大于 5% 阈值，需要触发混合回收；且根据预测，可以留给 Old Gen 的回收时间非常短（不足 1ms），导致选择选择 CSet 时 Old Gen region 特别少，剩余 region 仍然大于 5%阈值，因此 mixed 阶段将持续，然后恶性循环（最多 8 次 mixed gc）；中间如果出发 YGC，则 humongous 对象会被清除，Old Gen 占用会大幅下降（真正的老年代对象并不会清除）； 问题在于预测时间为什么会这么短？ 优化方案 优化 PrometheusScrapeEndpoint 相关讨论：\nStringHttpMessageConverter may trigger humongous allocations (G1GC) #25645 Add support for streaming responses from actuator web endpoints #21308 思路？\n使用一个 char[] 内存池，scrape 时从内存池获取一个足够大的 char 数组用于生成数据，完成后，将数组归还内存池？ 减少 tag 数量？ ","date":"2023-06-26T13:30:37+08:00","permalink":"https://zhumengzhu.github.io/2023/06/java-service-old-generation-memory-anomaly-large-object-impact-and-solutions/","title":"Java 服务老年代内存异常：大对象的影响与解决方案"},{"content":"参数建议 这些建议基于以下前提：\n服务器 CPU 支持超线程，即一个核可以当成两个核用，否则，GC 线程相关参数值应该减半； 如果 JDK 版本小于 1.8.0_191-b12(如JDK 1.8.0_151)，则需要设置 -XX:ParallelGCThreads=n, -XX:ConcGCThreads=n, -XX:G1ConcRefinementThreads=n 三个参数； 如果 JDK 版本升级到 1.8.0_191-b12 及以上，则最好不要设置上述三个参数，改为设置 -XX:+UnlockExperimentalVMOptions -XX:ActiveProcessorCount=n，n 取容器核数； 统一设置的参数 -XX:+UseG1GC 使用垃圾优先(G1)收集器 -XX:+PrintFlagsFinal 打印所有参数的最终值（参考：Java -XX:+PrintFlagsFinal命令行参数详解） -XX:+PrintCommandLineFlags 是打印那些被新值覆盖的项 -XX:+PrintStringTableStatistics 在 JVM 进程退出时会输出 SymbolTable 统计和 StringTable 统计（参考：聊聊jvm的StringTable及SymbolTable） -XX:+PrintGC 打印GC日志 -XX:+PrintGCDetails 打印详细的GC日志，还会在退出前打印堆的详细信息 -XX:+PrintGCDateStamps 打印CG发生的日期时间 -XX:+PrintGCTimeStamps 打印CG发生的相对时间戳 -XX:+PrintHeapAtGC 每次GC前后打印堆信息 -XX:+PrintTenuringDistribution YGC 时，打印出幸存区中对象的年龄分布 -XX:+PrintAdaptiveSizePolicy 打印分代大小调整的信息 -XX:+PrintGCApplicationStoppedTime 打印应用由于GC而产生的停顿时间 -XX:+PrintGCApplicationConcurrentTime 打印应用程序的执行时间 -XX:+PrintReferenceGC 打印软引用、弱引用、虚引用和Finallize队列处理情况 -XX:ParallelRefProcEnabled 启用并行引用处理 -XX:-OmitStackTraceInFastThrow 禁用 FastThrow 优化。如果开启，会导致抛出异常时没有堆栈信息，不利于排查问题。 -Xloggc:/app/logs/gc_$HOSTNAME.log GC 日志路径 -XX:GCLogFileSize=30M GC 日志文件大小 -XX:+UseGCLogFileRotation 启用 GC 日志文件滚动策略，需要先设置 -Xloggc -XX:NumberOfGCLogFiles=10 设置滚动日志时的文件数 -XX:+HeapDumpOnOutOfMemoryError 当发生 OOM 时，自动生成 dump 文件 -XX:HeapDumpPath=\u0026quot;/app/logs/java_%p_$HOSTNAME.hprof\u0026quot; 指定 dump 文件的路径 -XX:ErrorFile=/app/logs/hs_err_%p_$HOSTNAME.log 错误发生时，错误数据的存储路径 -Djava.security.egd=file:/dev/./urandom 加快随机数产生过程 允许调整的参数 -Xms2g -Xms4g total_memory / 2 最小堆内存，JVM 初始化时就会分配的堆内存大小。 -Xmx2g -Xms4g total_memory / 2 最大堆内存 -XX:MaxDirectMemorySize=512m -XX:MaxDirectMemorySize=512m - 设置最大直接内存（参考 JVM源码分析之堆外内存完全解读） -XX:MaxMetaspaceSize=512m -XX:MaxMetaspaceSize=512m - 设置最大元空间（参考JVM源码分析之Metaspace解密） -XX:MaxGCPauseMillis=200 -XX:MaxGCPauseMillis=200 - 设置最大GC暂停时间的目标。这是一个软目标，JVM将尽最大努力实现它。默认 200ms。 -XX:ParallelGCThreads=4 -XX:ParallelGCThreads=8 - 设置垃圾回收器并行阶段使用的线程数。计算公式见JVM调优系列: 默认GC线程数的计算公式 -XX:ConcGCThreads=1 -XX:ConcGCThreads=2 - 设置垃圾回收器并发阶段使用的线程数 -XX:G1ConcRefinementThreads=5 -XX:G1ConcRefinementThreads=9 ParallelGCThreads+1 设置并发优化线程，只专注扫描日志缓冲区记录的卡片来维护更新 RSet -XX:G1ReservePercent=10 -XX:G1ReservePercent=10 - 设置要保持空闲的预留内存的百分比，以减少空间溢出的风险。 默认值是10%。 当增加或减少这个百分比时，请确保对Java堆总量进行相同的调整。 -XX:InitiatingHeapOccupancyPercent=45 -XX:InitiatingHeapOccupancyPercent=45 - 启动并发GC周期的(整个)堆占用的百分比。 它由基于整个堆占用情况触发并发GC周期的GC使用，而不仅仅是其中的一个代(例如G1)。 值为0表示“执行恒定的GC周期”。 缺省值为45。 -XX:SoftRefLRUPolicyMSPerMB=1000 -XX:SoftRefLRUPolicyMSPerMB=1000 - 每1M空闲空间可保持的SoftReference对象生存的时长（单位ms）（参考：一次 JVM FullGC 的排查过程及解决方案！） 不建议设置的参数 不设置 -XX:+DisableExplicitGC，这样允许业务使用 System.gc() 在某些特定情况下主动 FullGC； 不要指定 -Xmn 参数，让 G1 自己去调整；如果设置了年轻代大小，会导致 G1 无法使用暂停时间目标； 完整参数示例 以下针对 4C8G 机器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 -Xms4g -Xmx4g -XX:MaxDirectMemorySize=512m -XX:MaxMetaspaceSize=512m -XX:+PrintFlagsFinal -XX:+PrintCommandLineFlags -XX:+PrintStringTableStatistics -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=2 -XX:G1ConcRefinementThreads=9 -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -XX:+PrintTenuringDistribution -XX:+PrintAdaptiveSizePolicy -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime -XX:G1ReservePercent=10 -XX:InitiatingHeapOccupancyPercent=45 -XX:SoftRefLRUPolicyMSPerMB=1000 -XX:+PrintReferenceGC -XX:ParallelRefProcEnabled -Xloggc:/app/logs/gc_$HOSTNAME.log -XX:GCLogFileSize=30M -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=\u0026#34;/app/logs/java_%p_$HOSTNAME.hprof\u0026#34; -XX:ErrorFile=/app/logs/hs_err_%p_$HOSTNAME.log -XX:-OmitStackTraceInFastThrow -Djava.security.egd=file:/dev/./urandom 其它 建议升级 JDK 至少到 1.8.0_191-b12 及以上，原因参考 JVM如何获取当前容器的资源限制？ 和 云原生架构：容器资源限制及资源可见性 参考资料 Java HotSpot VM Options 10 Garbage-First Garbage Collector Tuning Collecting and reading G1 garbage collector logs - part 2 JVM性能——JVM调优参数列表 详解 G1 垃圾收集器 JVM之G1回收器和常见参数配置 ","date":"2023-06-18T12:18:18+08:00","permalink":"https://zhumengzhu.github.io/2023/06/jvm-parameter-recommendations-and-reasons/","title":"JVM 参数建议及理由"},{"content":"引子 在启动 Java 项时，可能会出现这种诡异的现象：\n在本地可以启动，但部署到服务器上却不能启动； 或者，在服务器可以启动，但本地不可以启动； 此时如果去查看错误日志，往往可以看到诸如 ClassNotFoundException、NoClassDefFoundError 等异常信息，接着很容易就会想到很可能是发生了类冲突。 然后你会结合异常日志即依赖变更情况，定位到冲突的类及其所在的 jar 包，然后用 dependecy-exclude 等手段将冲突解决掉，最终使得服务可以正常启动。一般我们处理问题就到此为止了。 本文试图更近一步，深入分析本地与远程服务器行为不一致的原因，并简单介绍业界解决依赖冲突的一些方法，和排查类冲突问题的小技巧。\n背景知识 Java Classpath: 本地启动的 Classpath 顺序 本地一般都是在 IDEA 中启动；IDEA 启动服务时，会在 Console 打印启动参数： 那么 IDEA 是以怎样的顺序将 jar 包添加到 classpath 中呢？\n答案：是按 Maven 依赖解析的方式进行加载的。通过执行 mvn dependency:list 可以看到 Maven 依赖解析的结果，这个顺序，就是 classpath 中 jar 的添加顺序： 通过对比可以发现，两者顺序一致。\n远程启动的 Classpath 顺序 登录服务器，使用 ps aux | grep java 查看启动参数，会发现，远程服务器没有加 -classpath 参数！！！相关的依赖在哪里？？？ 实际上，在构建项目时，Maven 会将所有的依赖都添加到 xx-server.jar 中的 BOOT-INF/lib 目录下。\nJVM 在启动时，会根据 Jar 包中的另一个元数据文件\u0026ndash;META-INF/MANIFEST.MF，来决定类的加载顺序。\nMETA-INF/MANIFEST.MF 默认情况下，JVM 在加载类的顺序依赖 OS 时，对于 Linux 来说，最底层是 opendir 函数，这个函数返回的顺序，又与文件系统有关。『对于 CentOS 6，它使用的是Ext4，文件顺序与目录文件的大小是否超过一个磁盘块和文件系统计算的Hash值有关。』\n简单说，先加载完全是哪个看运气！远程服务器的版本不同，加载的顺序就可能不一样。这就是文章开头诡异问题的根源。\n看运气怎么行？！\nspring-boot-maven-plugin 这个插件使用了 The Executable Jar Format，支持一种称之为 Classpath Index 的索引文件，负责指定 jar 被添加到 classpath 中的顺序。很明显，通过这个插件可以保证 jar 的扫描顺序在不同的环境下是一致的。完美解决上面的问题。 小节 类的加载顺序取决于 classpath； 嵌套的 JARS 中的加载顺序，在默认情况取决于 OS，对于 Linux 来说取决于文件系统； Spring Boot 提供了一个插件，利用 The Executable Jar Format 完美解决了加载顺序错乱问题； 其它 业界的一些其他解决方案 Spring Boot 的方案可以解决拥有 main 方法的服务的冲突问题，解决方法依赖一个 Maven 差距。\n对于开发或扩展类库(比如 guava/hadoop)，或者 Java Agent 的开发时，有时候面临的问题更复杂，甚至要解决 classloader 冲突的问题。\nmaven-plugin-shade maven-plugin-shade 详解\n自定义类加载器 Java Agent 的类加载隔离实现\n思路：『使用独立的类加载器去加载 Java Agent 依赖的类，该独立的类加载器的 parent 指向 Bootstrap ClassLoader，且将 Java Agent 依赖的类的默认后缀 .class 进行调整，以避免系统类加载器加载到这些类，以实现类的隔离。』\n类冲突问题排查小技巧 加启动参数 -verbose:classpath 加上这个参数， JVM 会将『哪个类是从哪个 jar中被加载』的信息输出到 console 中。\n这个方法需要你能控制启动参数，适合在本地，不确定哪个类冲突的时候使用。\n使用 arthas 的 Jad 功能 arthas 输出的信息更全面，出了现实加载的位置，还会告诉你 ClassLoader 是哪一个，并且自动反编译(jad 命令本来就是干这个的！)\n这个方法需要你安装 arthas，适合本地和 dev、test 环境，在你有了明确的怀疑对象时，优先使用这个命令。\n如何安装 arthas 1 2 3 4 # https://arthas.aliyun.com/doc/install-detail.html curl -O https://arthas.aliyun.com/arthas-boot.jar java -jar arthas-boot.jar ","date":"2023-04-03T18:26:48+08:00","permalink":"https://zhumengzhu.github.io/2023/04/java-class-conflict-and-solutions/","title":"Java 类冲突问题及其解决方案"},{"content":"Contention in java.util.Random it\u0026rsquo;s thread-safe it\u0026rsquo;s lock-free To better understand this, let\u0026rsquo;s see how one of its primary operations, next(int), is implemented:\n1 2 3 4 5 6 7 8 9 protected int next(int bits) { long oldseed, nextseed; AtomicLong seed = this.seed; do { oldseed = seed.get(); nextseed = (oldseed * multiplier + addend) \u0026amp; mask; } while (!seed.compareAndSet(oldseed, nextseed)); return (int)(nextseed \u0026gt;\u0026gt;\u0026gt; (48 - bits)); } Instead of a dedicated instance of Random per thread, each thread only needs to maintain its own seed value. As of Java 8, the Thread class itself has been retrofitted to maintain the seed value:\n1 2 3 4 5 6 7 8 9 10 11 12 public class Thread implements Runnable { // omitted @jdk.internal.vm.annotation.Contended(\u0026#34;tlr\u0026#34;) long threadLocalRandomSeed; @jdk.internal.vm.annotation.Contended(\u0026#34;tlr\u0026#34;) int threadLocalRandomProbe; @jdk.internal.vm.annotation.Contended(\u0026#34;tlr\u0026#34;) int threadLocalRandomSecondarySeed; } The threadLocalRandomSeed variable is responsible for maintaining the current seed value for ThreadLocalRandom. Moreover, the secondary seed, threadLocalRandomSecondarySeed, is usually used internally by the likes of ForkJoinPool.\nThis implementation incorporates a few optimizations to make ThreadLocalRandom even more performant:\nAvoiding false sharing by using the @Contented annotation, which basically adds enough padding to isolate the contended variables in their own cache lines Using sun.misc.Unsafe to update these three variables instead of using the Reflection API Avoiding extra hashtable lookups associated with the ThreadLocal implementation Contention in java.util.SecureRandom it\u0026rsquo;s the subclass of java.util.Random\none of the most common locking issues within Java applications is triggered through an innocent-looking java.io.File.createTempFile() calls. Under the hood, this temporary file creation is relying upon a SecureRandom class to calculate the name of the file.\n1 2 3 4 5 6 7 8 9 10 private static final SecureRandom random = new SecureRandom(); static File generateFile(String prefix, String suffix, File dir) { long n = random.nextLong(); if (n == Long.MIN_VALUE) { n = 0; // corner case } else { n = Math.abs(n); } return new File(dir, prefix + Long.toString(n) + suffix); } And SecureRandom, when nextLong is called, eventually calls its method nextBytes(), which is defined as synchronized:\n1 2 3 synchronized public void nextBytes(byte[] bytes) { secureRandomSpi.engineNextBytes(bytes); } One may say, that if I create new SecureRandom in each thread, I will not get any issues. Unfortunately, it’s not that simple. SecureRandom uses an implementation of java.security.SecureRandomSpi, which will eventually be contended anyhow (you may look at the following bug discussion with some benchmarks in Jenkins issue tracker)\nThis in combination with certain application usage patterns (especially if you have lots of SSL connections which rely on SecureRandom for their crypto-handshaking magic) has a tendency to build up into long-lasting contention issues.\nThe fix to the situation is simple if you can control the source code – just rebuild the solution to rely upon the java.util.ThreadLocalRandom for multithreaded designs. In cases where you are stuck with a standard API making the decisions for you the solution can be more complex and require significant refactoring.\nurandom and random Devices https://www.ibm.com/docs/en/aix/7.2?topic=files-urandom-random-devices\nLinux Kernel 5.8 之前的版本，如果熵池的数据不足，从 /dev/random 读取数据可能阻塞，但是从 /dev/urandom 不会阻塞。Java 程序一般会指定 -Djava.security.egd=file:/dev/./urandom 参数避免阻塞的发生(但不一定生效)。 需要注意的是，\n-Djava.security.egd=file:/dev/urandom 是错误的； -Djava.security.egd=file:/dev/./urandom 才是正确的； 见：https://stackoverflow.com/questions/137212/how-to-deal-with-a-slow-securerandom-generator 关于性能 ThreadLocalRandom 和 SecureRandom 都是 Random 的子类； 性能：ThreadLocalRandom \u0026gt; Random \u0026gt; SecureRandom； Random 和 ThreadLocalRandom 使用的都是「线性同余发生器」算法，速度很快，区别是： Random 使用 CAS 方式更新种子，高并发情况下竞争大，性能会下降； ThreadLocalRandom 将种子存在在 Thread 对象上，性能更好： 种子封闭在线程内，不需要 CAS； 避免额外的『间接』计算： 相比 ThreadLocal + Random，在 Java8 中，ThreadLocalRandom 只需要直接读取存储在 Thread 对象上的 threadLocalRandomSeed 然后直接更新就可以，不需要调用 ThreadLocal.get 进行一次 HashTable Lookup； 通过 UNSAFE 的 native 方法更新种子值，而不是反射，效率更高； 通过使用 Contended 注解，避免伪共享问题； SecureRandom 使用 OS 维护的随机数据来生成随机数，底层会使用 synchronized 锁，因此性能比较差，且可能发生阻塞； SecureRandom 使用的某些算法，需要读取 /dev/random 来获取随机数据，当 OS 维护的熵池中的熵不足时，会发生阻塞，导致严重的性能问题； 在 Linux 的 v5.6 及其以后的内核中，读取 /dev/random 不会再被阻塞了，但是就像 /dev/urandom 一样，在一个全新的系统刚启动时由于系统尚未产生熵值，此时还是可能阻塞等待足够的熵值产生； UUID.randomUUID 底层使用的就是 SecureRandom，因此性能会比较差； File.createTempFile 底层也会使用 SecureRandom，因此性能会比较差； 建立 SSL 连接同样依赖 SecureRandom，因此也要小心； 如何预测下一个随机数 以下方法仅对基于 LCG 算法有效，利用公式 𝑋𝑛+1=(𝑎𝑋𝑛+𝑐) mod 𝑚。\n给定两个连续的 int 类型值，或一个 double，或一个 long 类型值，即可预测后续的随机数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 Predict Next Random Number.java import java.util.ArrayList; import java.util.Random; public class ReplicatedRandom extends Random { public static void main(String[] args) { predictByNextDouble(); predictByNextInt(); } private static void predictByNextInt() { long seed = System.currentTimeMillis(); Random r = new Random(seed); int v1 = r.nextInt(); int v2 = r.nextInt(); ReplicatedRandom rr = new ReplicatedRandom(); rr.replicateState(v1, v2); System.out.println(r.nextInt() == rr.nextInt()); // True System.out.println(r.nextInt() == rr.nextInt()); // True System.out.println(r.nextInt() == rr.nextInt()); // True } private static void predictByNextDouble() { long seed = System.currentTimeMillis(); Random r = new Random(seed); ReplicatedRandom rr = new ReplicatedRandom(); rr.replicateState(r.nextDouble()); System.out.println(r.nextDouble() == rr.nextDouble()); // True System.out.println(r.nextDouble() == rr.nextDouble()); // True System.out.println(r.nextDouble() == rr.nextDouble()); // True } // Replicate the state of a Random using a single value from its nextDouble public boolean replicateState(double nextDouble) { // nextDouble() is generated from ((next(26) \u0026lt;\u0026lt; 27) + next(27)) / (1L \u0026lt;\u0026lt; 53) // Inverting those operations will get us the values of next(26) and next(27) long numerator = (long) (nextDouble * (1L \u0026lt;\u0026lt; 53)); int first26 = (int) (numerator \u0026gt;\u0026gt;\u0026gt; 27); int last27 = (int) (numerator \u0026amp; ((1L \u0026lt;\u0026lt; 27) - 1)); return replicateState(first26, 26, last27, 27); } // Replicate the state of a Random using a single value from its nextLong public boolean replicateState(long nextLong) { int last32 = (int) (nextLong \u0026amp; ((1L \u0026lt;\u0026lt; 32) - 1)); int first32 = (int) ((nextLong - last32) \u0026gt;\u0026gt; 32); return replicateState(first32, 32, last32, 32); } // Replicate the state of a Random using two consecutive values from its nextInt public boolean replicateState(int firstNextInt, int secondNextInt) { return replicateState(firstNextInt, 32, secondNextInt, 32); } // Replicate the state of a Random using two consecutive values from its nextFloat public boolean replicateState(float firstNextFloat, float secondNextFloat) { return replicateState((int) (firstNextFloat * (1 \u0026lt;\u0026lt; 24)), 24, (int) (secondNextFloat * (1 \u0026lt;\u0026lt; 24)), 24); } public boolean replicateState(int nextN, int n, int nextM, int m) { // Constants copied from java.util.Random final long multiplier = 0x5DEECE66DL; final long addend = 0xBL; final long mask = (1L \u0026lt;\u0026lt; 48) - 1; long upperMOf48Mask = ((1L \u0026lt;\u0026lt; m) - 1) \u0026lt;\u0026lt; (48 - m); // next(x) is generated by taking the upper x bits of 48 bits of (oldSeed * multiplier + addend) mod (mask + 1) // So now we have the upper n and m bits of two consecutive calls of next(n) and next(m) long oldSeedUpperN = ((long) nextN \u0026lt;\u0026lt; (48 - n)) \u0026amp; mask; long newSeedUpperM = ((long) nextM \u0026lt;\u0026lt; (48 - m)) \u0026amp; mask; // Bruteforce the lower (48 - n) bits of the oldSeed that was truncated. // Calculate the next seed for each guess of oldSeed and check if it has the same top m bits as our newSeed. // If it does then the guess is right and we can add that to our candidate seeds. ArrayList\u0026lt;Long\u0026gt; possibleSeeds = new ArrayList\u0026lt;Long\u0026gt;(); for (long oldSeed = oldSeedUpperN; oldSeed \u0026lt;= (oldSeedUpperN | ((1L \u0026lt;\u0026lt; (48 - n)) - 1)); oldSeed++) { long newSeed = (oldSeed * multiplier + addend) \u0026amp; mask; if ((newSeed \u0026amp; upperMOf48Mask) == newSeedUpperM) { possibleSeeds.add(newSeed); } } if (possibleSeeds.size() == 1) { // If there\u0026#39;s only one candidate seed, then we found it! setSeed(possibleSeeds.get(0) ^ multiplier); // setSeed(x) sets seed to `(x ^ multiplier) \u0026amp; mask`, so we need another `^ multiplier` to cancel it out return true; } if (possibleSeeds.size() \u0026gt;= 1) { System.out.println(\u0026#34;Didn\u0026#39;t find a unique seed. Possible seeds were: \u0026#34; + possibleSeeds); } else { System.out.println(\u0026#34;Failed to find seed!\u0026#34;); } return false; } } 说明：\n上面的算法利用了 Java 生成的随机数有 48 个有效位这一信息； 对于 nextInt 来说，返回值是 32 位，因此在返回结果时，会舍弃低 16 位，这是通过 \u0026raquo;\u0026gt; 位移操作实现的； 对于 nextLong 来说，它是由两个连续的 nextInt 组成的； 对于 nextDouble 来说，它是由两个连续的分别是 27 bits 和 26 bits 数字组成的； 因此，当给定一个 nextInt 值时，将其左移 16 位，即得到种子的最小可能值，该值加上 2^16 - 1，就是种子的最大可能值；因此，只需遍历该空间(包含 2^16 个数字)，即可推测出种子的实际值； 现代 CPU 可以在 1 秒内用暴力方法逆向的推测出当前的种子值； Benchmark 单线程结果： 4 线程结果： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 RandomBenchmark.java package benchmark; import org.openjdk.jmh.annotations.Benchmark; import org.openjdk.jmh.annotations.BenchmarkMode; import org.openjdk.jmh.annotations.Fork; import org.openjdk.jmh.annotations.Measurement; import org.openjdk.jmh.annotations.Mode; import org.openjdk.jmh.annotations.OutputTimeUnit; import org.openjdk.jmh.annotations.Scope; import org.openjdk.jmh.annotations.State; import org.openjdk.jmh.annotations.Threads; import org.openjdk.jmh.annotations.Warmup; import org.openjdk.jmh.results.format.ResultFormatType; import org.openjdk.jmh.runner.Runner; import org.openjdk.jmh.runner.RunnerException; import org.openjdk.jmh.runner.options.Options; import org.openjdk.jmh.runner.options.OptionsBuilder; import java.time.LocalDateTime; import java.time.format.DateTimeFormatter; import java.util.Random; import java.util.concurrent.ThreadLocalRandom; import java.util.concurrent.TimeUnit; /** */ @BenchmarkMode({Mode.AverageTime, Mode.Throughput}) @Warmup(iterations = 3, time = 1) @Measurement(iterations = 5, time = 5) @Threads(4) @Fork(1) @State(value = Scope.Benchmark) @OutputTimeUnit(TimeUnit.NANOSECONDS) public class RandomBenchmark { private final Random random = new Random(); private final ThreadLocal\u0026lt;Random\u0026gt; simpleThreadLocal = ThreadLocal.withInitial(Random::new); public static void main(String[] args) throws RunnerException { // 将结果文件上传到 https://jmh.morethan.io/ 可以得到可视化结果 String now = LocalDateTime.now().format(DateTimeFormatter.ofPattern(\u0026#34;yyyyMMddHHmmss\u0026#34;)); String filePath = \u0026#34;/tmp/jmh_result_\u0026#34; + now + \u0026#34;.json\u0026#34;; Options opt = new OptionsBuilder() .include(RandomBenchmark.class.getSimpleName()) .result(filePath) .resultFormat(ResultFormatType.JSON).build(); new Runner(opt).run(); } @Benchmark // @BenchmarkMode(Throughput) public int regularRandom() { return random.nextInt(); } @Benchmark // @BenchmarkMode(Throughput) public int simpleThreadLocal() { return simpleThreadLocal.get().nextInt(); } @Benchmark // @BenchmarkMode(Throughput) public int builtinThreadLocal() { return ThreadLocalRandom.current().nextInt(); } } Reference https://www.baeldung.com/java-thread-local-random https://alidg.me/blog/2020/4/24/thread-local-random https://stackoverflow.com/questions/11051205/difference-between-java-util-random-and-java-security-securerandom https://resources.infosecinstitute.com/topic/random-number-generation-java/ https://jazzy.id.au/2010/09/21/cracking_random_number_generators_part_2.html https://jazzy.id.au/2010/09/22/cracking_random_number_generators_part_3.html https://blogs.oracle.com/linux/post/rngd1 https://blogweb.cn/article/1642969777211 https://lwn.net/Articles/808575/ https://unix.stackexchange.com/questions/243127/how-to-check-if-reading-from-dev-random-will-block https://crypto.stackexchange.com/questions/51686/how-to-determine-the-next-number-from-javas-random-method https://franklinta.com/2014/08/31/predicting-the-next-math-random-in-java/ https://jazzy.id.au/2010/09/20/cracking_random_number_generators_part_1.html (讲解了破解随机数生成器的理论基础) ","date":"2023-01-30T10:33:47+08:00","permalink":"https://zhumengzhu.github.io/2023/01/java-random-number-issues-overview/","title":"Java 随机数问题综述"},{"content":"说明 说明\n原文：https://dev.mysql.com/doc/refman/8.0/en/innodb-locks-set.html 下文黑色字体原文，黄色字体是翻译，红色是自己加的标注，记录自己对原文的理解。 测试表结构： 1 2 3 4 5 6 CREATE TABLE `t` ( `id` bigint NOT NULL AUTO_INCREMENT COMMENT \u0026#39;主键 ID\u0026#39;, `version` bigint NOT NULL DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;版本号\u0026#39;, PRIMARY KEY (`id`), UNIQUE KEY `uk_version` (`version`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci COMMENT=\u0026#39;版本表\u0026#39;； 表里的初始数据为：\n1 1 2 5 3 10 4 15 翻译\u0026amp;笔记 『A locking read, an UPDATE, or a DELETE generally set record locks on every index record that is scanned in the processing of an SQL statement. It does not matter whether there are WHERE conditions in the statement that would exclude the row. InnoDB does not remember the exact WHERE condition, but only knows which index ranges were scanned. The locks are normally next-key locks that also block inserts into the “gap” immediately before the record. However, gap locking can be disabled explicitly, which causes next-key locking not to be used. For more information, see Section 15.7.1, “InnoDB Locking”. The transaction isolation level can also affect which locks are set; see Section 15.7.2.1, “Transaction Isolation Levels”.』\n一次加锁的读、更新或删除通常在 SQL 语句处理过程中扫描的每个索引记录上设置记录锁。 语句中是否使用 WHERE 条件排除了某些行并不关键。InnoDB 不会记住准确的 WHERE 条件，而是只关心索引的扫描范围。锁通常是临键锁，用来组织在记录之前的『间隙』插入记录。但是，间隙锁可以被禁用，此时不会使用临键锁。更多信息，见第15.7.1，『InnoDB Locking』。事务隔离级别也会影响锁的设置，详情见15.7.2.1，『Transaction Isolation Levels』。 锁是设置在索引记录上的。 『If a secondary index is used in a search and the index record locks to be set are exclusive, InnoDB also retrieves the corresponding clustered index records and sets locks on them.』\n如果在搜索中使用了二级索引，并且要设置的索引记录锁是排他的，InnoDB也会检索相应的聚集索引记录并对其设置锁。 怎么理解？ 『在搜索中使用二级索引』，InnoDB 在执行时是通过扫描二级索引来查找记录的，举个例子： 事务一，执行 begin; select * from t where version = 1 for update; 事务二，执行 select id from t where id = 1 for update; 会被阻塞；此时查询data_locks 表，会发现事务一在聚簇索引上设置了记录锁： 『If you have no indexes suitable for your statement and MySQL must scan the entire table to process the statement, every row of the table becomes locked, which in turn blocks all inserts by other users to the table. It is important to create good indexes so that your queries do not scan more rows than necessary.』\n如果没有适合语句的索引，MySQL必须扫描整个表来处理语句，此时表的每一行都被锁定，从而阻止其他用户对表的所有插入。 创建良好的索引非常重要，这样查询就不会扫描不必要的行。” 『InnoDB sets specific types of locks as follows.』\nInnoDB设置锁的具体类型如下。 『SELECT \u0026hellip; FROM is a consistent read, reading a snapshot of the database and setting no locks unless the transaction isolation level is set to SERIALIZABLE. For SERIALIZABLE level, the search sets shared next-key locks on the index records it encounters. However, only an index record lock is required for statements that lock rows using a unique index to search for a unique row.』 『When UPDATE modifies a clustered index record, implicit locks are taken on affected secondary index records. The UPDATE operation also takes shared locks on affected secondary index records when performing duplicate check scans prior to inserting new secondary index records, and when inserting new secondary index records.』 当 Update 修改聚簇索引记录时，会隐式的对受影响的次级索引加锁。在插入新的次级索引记录之前执行重复检查扫描时，以及在插入新的次级索引记录时，UPDATE 操作还对受影响的次级索引记录加共享锁。 怎么理解 『隐式加锁』？ 事务一，执行 begin; update t set version = -1 where id = 1; 然后查 data_locks 表，会发现没有对 uk_version 加锁； 事务二，执行 update t set version = -2 where version = 1; 会被阻塞，此时再次查看 data_locks 表，会发现事务一获取了 uk_version 的记录锁： 这说明，事务一确实会对次级索引加锁，只是不知道什么原因，没有锁冲突是，在 data_locks 表中查不到锁记录； 怎么理解『进行重复性检查扫描时，会对受影响的次级索引加共享锁』？ 执行 begin; insert into t (version) value (1); 然后查 data_locks 表，会发现事务持有了 uk_version 上的 S 锁； 由于表中已经有一条 version=1 的记录，当在插入 version=1 的记录时，会加一个 S 锁以进行重复检查； ","date":"2023-01-16T11:07:26+08:00","permalink":"https://zhumengzhu.github.io/2023/01/locks-set-by-different-sql-statements-in-innodb/","title":"『Locks Set by Different SQL Statements in InnoDB』翻译\u0026笔记"},{"content":"目标 一：分析下面这条语句执行时，会加哪些锁、为什么加这些锁，以及可能会存在什么问题：\nINSERT INTO t (version) SELECT IFNULL(MAX(version) + 1, 1) FROM t; 二：掌握基本的分析 MySQL 加锁情况的技能；\n结论 加的锁包括：\n两个意向锁 意向排他锁( IX 锁)： 事务想要获得一张表中某几行的排他锁 意向共享锁( IS 锁)：事务想要获得一张表中某几行的共享锁 三个共享锁 一个对 “上确界伪纪录”的共享临键锁； 一个对 “version值最大的那条uk_version索引记录”的共享临键锁； 一个对\u0026quot;待插入的数据的uk_version索引记录\u0026quot;的间隙锁； 下表是从 performance_schema.data_locks 表中截取的真实数据(省略了不相关的字段)：\nINDEX_NAME LOCK_TYPE LOCK_MODE LOCK_DATA 说明 NULL TABLE IX NULL NULL TABLE IS NULL uk_version RECORD S supremum pseudo-record 对“上确界伪纪录”加 shared next-key locks uk_version RECORD S 15,4 对 version 最大的那条记录加 next-key locks uk_version RECORD S,GAP 16, 5 验证 数据库版本 进行实验的 MySQL 版本为: 8.0.27 MySQL Community Server - GPL\n库表准备 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 创建库 CREATE DATABASE IF NOT EXISTS `test`; USE `test`; # 创建表 CREATE TABLE IF NOT EXISTS `t` ( `id` BIGINT AUTO_INCREMENT COMMENT \u0026#39;主键 ID\u0026#39;, `version` BIGINT DEFAULT 0 NOT NULL COMMENT \u0026#39;版本号\u0026#39;, PRIMARY KEY (`id`), UNIQUE KEY `uk_version` (`version`) ) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4 COMMENT \u0026#39;版本表\u0026#39;; # 初始化数据 INSERT INTO `t` (`version`) VALUE (1); INSERT INTO `t` (`version`) VALUE (5); INSERT INTO `t` (`version`) VALUE (10); INSERT INTO `t` (`version`) VALUE (15); 实验 实验一 INSERT INTO t (version) SELECT IFNULL(MAX(version) + 1, 1) FROM t`` 直接执行 INSERTO \u0026hellip; SELECT 然后看加锁情况。\n1 2 3 4 5 6 7 8 9 # session 1 BEGIN; INSERT INTO `t` (`version`) SELECT IFNULL(MAX(version) + 1, 1) FROM `t`; # session 2 SELECT ENGINE, ENGINE_LOCK_ID, ENGINE_TRANSACTION_ID, THREAD_ID, INDEX_NAME, OBJECT_INSTANCE_BEGIN, LOCK_TYPE, LOCK_MODE, LOCK_STATUS, LOCK_DATA FROM performance_schema.data_locks; # session 1 ROLLBACK; MySQL 官方文档中，大概解释了为什么加这些锁：\nhttps://dev.mysql.com/doc/refman/8.0/en/innodb-locks-set.html INSERT INTO T SELECT \u0026hellip; FROM S WHERE \u0026hellip; sets an exclusive index record lock (without a gap lock) on each row inserted into T. If the transaction isolation level is READ COMMITTED, InnoDB does the search on S as a consistent read (no locks). Otherwise, InnoDB sets shared next-key locks on rows from S. InnoDB has to set locks in the latter case: During roll-forward recovery using a statement-based binary log, every SQL statement must be executed in exactly the same way it was done originally. CREATE TABLE \u0026hellip; SELECT \u0026hellip; performs the SELECT with shared next-key locks or as a consistent read, as for INSERT \u0026hellip; SELECT. When a SELECT is used in the constructs REPLACE INTO t SELECT \u0026hellip; FROM s WHERE \u0026hellip; or UPDATE t \u0026hellip; WHERE col IN (SELECT \u0026hellip; FROM s \u0026hellip;), InnoDB sets shared next-key locks on rows from table s.\n实验二 SELECT IFNULL(MAX(version) + 1, 1) AS version FROM t FOR SHARE; 单独执行 SELECT FOR SHARE 然后查看加锁情况：\n1 2 3 4 5 6 7 8 9 # session 1 BEGIN; SELECT IFNULL(MAX(version) + 1, 1) AS version FROM `t` FOR SHARE; # session 2 SELECT ENGINE, ENGINE_LOCK_ID, ENGINE_TRANSACTION_ID, THREAD_ID, INDEX_NAME, OBJECT_INSTANCE_BEGIN, LOCK_TYPE, LOCK_MODE, LOCK_STATUS, LOCK_DATA FROM performance_schema.data_locks; # session 1 ROLLBACK; 实验三 先 SELECT FOR SHARE 然后执行 INSERT.\n可以看到，在执行完 INSERT 后， 获取的锁与实验一相同。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # session 1 BEGIN; SELECT IFNULL(MAX(version) + 1, 1) AS version FROM `t` FOR SHARE; # session 2 SELECT ENGINE, ENGINE_LOCK_ID, ENGINE_TRANSACTION_ID, THREAD_ID, INDEX_NAME, OBJECT_INSTANCE_BEGIN, LOCK_TYPE, LOCK_MODE, LOCK_STATUS, LOCK_DATA FROM performance_schema.data_locks; # session 1 INSERT INTO `t` (`version`) VALUE (16); # session 2 SELECT ENGINE, ENGINE_LOCK_ID, ENGINE_TRANSACTION_ID, THREAD_ID, INDEX_NAME, OBJECT_INSTANCE_BEGIN, LOCK_TYPE, LOCK_MODE, LOCK_STATUS, LOCK_DATA FROM performance_schema.data_locks; # session 1 ROLLBACK; 加锁分析 对于上面的数据，其uk_version的结构大概如下：\n在执行 SELECT IFNULL(MAX(version) + 1, 1) AS version FROM t FOR SHARE 时：\n首先会先扫描到 (version=15, id=4) 这条记录，然后对其加 next-key lock；然后继续向后扫描，对 supremum pseudo-record 这条伪记录加 next-key lock。\n小节 结论一：INSERT INTO \u0026hellip; SELECT FROM 等价于先执行 SELECT FROM FOR SHARE 然后再 INSERT INTO； 结论二：SELECT MAX(\u0026hellip;) FROM \u0026hellip; 是范围查询，因此会加间隙锁； 结论三：结论二对唯一索引同样适用； 理论 锁分类 data_locks 表 官方文档：https://dev.mysql.com/doc/mysql-perfschema-excerpt/8.0/en/performance-schema-data-locks-table.html\n示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 MySQL [test]\u0026gt; SELECT * FROM performance_schema.data_locks\\G *************************** 1. row *************************** ENGINE: INNODB ENGINE_LOCK_ID: 281472727006424:1069:281472627977064 ENGINE_TRANSACTION_ID: 2423 THREAD_ID: 53 EVENT_ID: 129 OBJECT_SCHEMA: test OBJECT_NAME: t PARTITION_NAME: NULL SUBPARTITION_NAME: NULL INDEX_NAME: NULL OBJECT_INSTANCE_BEGIN: 281472627977064 LOCK_TYPE: TABLE LOCK_MODE: IX LOCK_STATUS: GRANTED LOCK_DATA: NULL *************************** 2. row *************************** ENGINE: INNODB ENGINE_LOCK_ID: 281472727006424:1069:281472627976976 ENGINE_TRANSACTION_ID: 2423 THREAD_ID: 53 EVENT_ID: 128 OBJECT_SCHEMA: test OBJECT_NAME: t PARTITION_NAME: NULL SUBPARTITION_NAME: NULL INDEX_NAME: NULL OBJECT_INSTANCE_BEGIN: 281472627976976 LOCK_TYPE: TABLE LOCK_MODE: IS LOCK_STATUS: GRANTED LOCK_DATA: NULL *************************** 3. row *************************** ENGINE: INNODB ENGINE_LOCK_ID: 281472727006424:4:5:1:281472627973984 ENGINE_TRANSACTION_ID: 2423 THREAD_ID: 53 EVENT_ID: 128 OBJECT_SCHEMA: test OBJECT_NAME: t PARTITION_NAME: NULL SUBPARTITION_NAME: NULL INDEX_NAME: uk_version OBJECT_INSTANCE_BEGIN: 281472627973984 LOCK_TYPE: RECORD LOCK_MODE: S LOCK_STATUS: GRANTED LOCK_DATA: supremum pseudo-record *************************** 4. row *************************** ENGINE: INNODB ENGINE_LOCK_ID: 281472727006424:4:5:5:281472627973984 ENGINE_TRANSACTION_ID: 2423 THREAD_ID: 53 EVENT_ID: 128 OBJECT_SCHEMA: test OBJECT_NAME: t PARTITION_NAME: NULL SUBPARTITION_NAME: NULL INDEX_NAME: uk_version OBJECT_INSTANCE_BEGIN: 281472627973984 LOCK_TYPE: RECORD LOCK_MODE: S LOCK_STATUS: GRANTED LOCK_DATA: 15, 4 *************************** 5. row *************************** ENGINE: INNODB ENGINE_LOCK_ID: 281472727006424:4:5:6:281472627974328 ENGINE_TRANSACTION_ID: 2423 THREAD_ID: 53 EVENT_ID: 129 OBJECT_SCHEMA: test OBJECT_NAME: t PARTITION_NAME: NULL SUBPARTITION_NAME: NULL INDEX_NAME: uk_version OBJECT_INSTANCE_BEGIN: 281472627974328 LOCK_TYPE: RECORD LOCK_MODE: S,GAP LOCK_STATUS: GRANTED LOCK_DATA: 16, 7 5 rows in set (0.004 sec) 字段说明 意向锁的作用\n当我们准备给一张表加上表锁的时候，我们首先要去 判断有没其他的事务锁定了其中了某些行？如果有的话，肯定不能加上表锁。那么这个时候我们就要去扫描整张表才能确定能不能成功加上一个表锁，如果数据量特别大，比如有上千万的数据的时候，加表锁的效率会很低。当我们在使用共享行锁时，Innodb 会自动给我们加上IS，使用排他行锁时自动加上IX ，用来表示改表中已经存在那些锁 参考：https://www.jianshu.com/p/ae91c63e7257\n插入意向锁的作用\n插入意向锁是在插入之前，先判断插入的间隙是否存在间隙锁，如果存在则产生一个插入意向锁，去等待间隙锁的释放。 多个事务插入同一个间隙的不同位置，他们并不会冲突。假设存在索引记录，其值分别为5和9。单独的事务分别尝试插入值6和7，在获得插入行的排他锁之前，每个事务都使用插入意图锁来锁定5和9之间的间隙，但他们不会互相阻塞。 参考官方说明：https://dev.mysql.com/doc/refman/8.0/en/innodb-locking.html An insert intention lock is a type of gap lock set by INSERT operations prior to row insertion. This lock signals the intent to insert in such a way that multiple transactions inserting into the same index gap need not wait for each other if they are not inserting at the same position within the gap. Suppose that there are index records with values of 4 and 7. Separate transactions that attempt to insert values of 5 and 6, respectively, each lock the gap between 4 and 7 with insert intention locks prior to obtaining the exclusive lock on the inserted row, but do not block each other because the rows are nonconflicting.\n为什么 INSERT INTO T SELECT \u0026hellip; FROM S WHERE \u0026hellip; 需要加共享锁 考虑下面的情况：\n初始时，表中的 version 的最大值是 15； 时间 事务一 事务二 T1 BEGIN; T2 INSERT INTO t (version) SELECT IFNULL(MAX(version) + 1, 1) FROM t; T3 (成功，写入一条 version=16 的数据) BEGIN; T4 INSERT INTO t (version) VALUE (10000); T5 COMMIT; T6 COMMIT; 在基于 statement-based binary log 的主从同步模式下，如果不加共享锁，由于事务二是先提交的，因此，bin log 在从库执行的时候，可能出现下面的情况：\n对于主库：事务一先写入一条 version=16的记录，事务二后写入一条 version=10000 的记录； 对于从库：事务二先写入一条 version=10000 的记录，事务一后写入一条 version=10001 的记录；✘ 库 记录 预期结果 实际结果 主库 记录一 version=16 version=16 主库 记录二 version=10000 version=10000 从库 记录一 version=16 version=10000 从库 记录二 version=10000 version=10001 Reference https://dev.mysql.com/doc/refman/8.0/en/innodb-locks-set.html https://dev.mysql.com/doc/refman/8.0/en/innodb-locking.html https://dev.mysql.com/doc/refman/8.0/en/innodb-locking-reads.html https://forums.mysql.com/read.php?22,689892,689892#msg-689892 MySQL8.0之锁事 MySQL锁的总结 和 一次插入意向锁的死锁还原分析 ","date":"2022-12-26T11:47:21+08:00","permalink":"https://zhumengzhu.github.io/2022/12/insert-into-t-select-from-s-where-conditional-query/","title":"INSERT INTO T SELECT ... FROM S WHERE ... 语句加锁分析"},{"content":"简介 本文从『优惠券发放』问题的讨论出发，介绍几个解决方案并分析其优缺点，然后讨论一下常见的update just one unused row模式，最后聊一下 MySQL 8.0.1 的 SKIP LOCKED 和 NOWAIT 特性。\n『券发放』问题 券发放，一般使用『预生成』模式。在发放前，先将券全部生成好，存在一张全表，这张表一般至少包含三个字段：\n一个全局唯一的券码字段； 一个状态字段表示是否发放； 一个用户 ID 字段记录券发给了谁； 当用户请求领券时，会从表中选择一条状态为『未领取』的记录，将其状态置为『已领取』，用户 ID置为『领券者的 ID』，最后将券码返给用户。\n假设券表结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 CREATE TABLE `coupon` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;主键id\u0026#39;, `code` VARCHAR(64) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;唯一劵码\u0026#39;, `status` TINYINT(10) NOT NULL DEFAULT 0 COMMENT \u0026#39;劵状态, 0: 未领取，1：已领取，2：已核销\u0026#39;, `user_id` BIGINT(20) NOT NULL DEFAULT 0 COMMENT \u0026#39;用户 ID\u0026#39;, `create_time` DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP, `update_time` DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`), UNIQUE KEY `uk_code` (`code`), KEY `idx_status` (`status`) ) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4 COMMENT =\u0026#39;优惠券表\u0026#39;; 券表中的数据如下：\nID CODE status user_id 1 c1 0 0 2 c2 0 0 3 c3 0 0 4 c4 0 0 5 c5 0 0 假设领券的用户 ID 为 1000。\n方法一：乐观锁-CAS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 1、选择一张券，假设选中的券码为 `c1`； SELECT * FROM coupon WHERE `status` = 0 LIMIT 1; # 2、修改券记录的状态 UPDATE coupon SET `status` = 1, `user_id` = 1000 WHERE `code` = \u0026#39;c1\u0026#39; AND `status` = 0; # 3、返回券码 `c1` 上述方法，在并发情况下可能存在问题：\n操作顺序 线程一 线程二 1 SELECT * FROM coupon WHERE status = 0 LIMIT 1; # 假设返回记录券码为 c1 2 SELECT * FROM coupon WHERE status = 0 LIMIT 1; # 假设返回记录券码也是 c1 3 UPDATE coupon SET status = 1, user_id = 1000 WHERE code = \u0026lsquo;c1\u0026rsquo; AND status = 0; # 成功 4 UPDATE coupon SET status = 1, user_id = 1000 WHERE code = \u0026lsquo;c1\u0026rsquo; AND status = 0; # 失败 由于线程一执行完第三步后，已将c1这条记录的status的值改为 1，因此线程二执行第四步会失败。所以需要重试。\n这可以看做一种『乐观锁』模式，『乐观锁』不适合高并发场景。\n方法二：悲观锁-SELECT FOR UPDATE 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # 1、开启事务（InnoDB 默认的 RR 级别） begin; # 2、选择一张券，假设选中的券码为 `c1`； SELECT * FROM coupon WHERE `status` = 0 LIMIT 1 FOR UPDATE; # 3、修改券记录的状态 UPDATE coupon SET `status` = 1, `user_id` = 1000 WHERE `code` = \u0026#39;c1\u0026#39; AND `status` = 0; # 4、提交事务 commit; # 5、返回券码 `c1` 为了解决并发的情况下某个操作失败， 可以开启一个事务，然后使用 select for update 先对记录进行加锁，然后再修改记录状态。 假设操作顺序如下：\n操作顺序 线程一 线程二 1 begin; 2 begin; 3 SELECT * FROM coupon WHERE status = 0 LIMIT 1 FOR UPDATE; # 假设返回记录券码为 c1 4 SELECT * FROM coupon WHERE status = 0 LIMIT 1 FOR UPDATE; # 阻塞 5 UPDATE coupon SET status = 1, user_id = 1000 WHERE code = \u0026lsquo;c1\u0026rsquo; AND status = 0; # 成功 6 commit; # 释放记录c1的锁 7 SELECT * FROM coupon WHERE status = 0 LIMIT 1; # 成功，返回券码 c2 8 UPDATE coupon SET status = 1, user_id = 1000 WHERE code = \u0026lsquo;c2\u0026rsquo; AND status = 0; # 成功 9 commit; # 释放记录c2的锁 线程二在执行第 4 步时，由于线程一已经抢到记录c1的锁，线程二会被阻塞，直到线程一提交事务（ 根据2PL，提交事务时才会释放锁）；线程二在 7 步获取锁后，由于记录c1的status的值已被线程一改为 1，所以此时查询到的记录是c2。\n方法二虽然可以保证券发放不会失败，但在高并发下，会有大量线程阻塞。\n方法三：使用 LAST_INSERT_ID 优化锁的持有时间 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 1、将状态为未领取的一张券发放给用户 UPDATE coupon SET `status` = 1, `user_id` = 1000, `id` = LAST_INSERT_ID(`id`) WHERE `status` = 0 LIMIT 1; # 2、查询步骤 1 修改的券的 ID，假设 id 为 1 SELECT LAST_INSERT_ID(); # 3、查询券码，假设券码值为 `c1` SELECT `code` FROM coupon WHERE id = 1; # 4、返回券码 `c1` 方法二中，根据 2PL（两阶段锁协议），锁的持有时间从 select for update 一直持续到 commit。 通过使用 LAST_INSERT_ID 可以避免开启事务，锁的持有时间非常短。\n不过这个方法没法避免锁争抢，因为根据 InnoDB 的锁机制，在扫描到满足条件的记录时会进行加锁(由于 idx_status 是非唯一索引，因此会加 next-key 锁) ，所以并发执行时仍然会存在阻塞的情况。\n这其实就是经典的**『热点数据更新』**问题。\n此外，要这个方法是 multi-user safe 的，所以在并发情况下也是安全的:\nIt is multi-user safe because multiple clients can issue the UPDATE statement and get their own sequence value with the SELECT statement (or mysql_insert_id()), without affecting or being affected by other clients that generate their own sequence values.\nsee MySQL 8.0 Reference Manual\n最后，还可以对上面的 SQL 进一步优化，使得只需执行 2 次 SQL：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 1、将状态为未领取的一张券发放给用户 UPDATE coupon SET `status` = 1, `user_id` = 1000, `id` = LAST_INSERT_ID(`id`) WHERE `status` = 0 LIMIT 1; # 2、查询券码 SELECT `code` FROM coupon, (SELECT LAST_INSERT_ID() id) AS t WHERE coupon.id = t.id; # 3、返回券码 `c1` 方法四：使用 SKIP LOCKED 避免锁争抢 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # 1、开启事务（InnoDB 默认的 RR 级别） begin; # 2、是用 `SELECT FOR UPDATE SKIP LOCKED` 选择一条券 SELECT * FROM coupon WHERE `status` = 0 LIMIT 1 FOR UPDATE SKIP LOCKED; # 3、修改券记录的状态 UPDATE coupon SET `status` = 1, `user_id` = 1000 WHERE `code` = \u0026#39;c1\u0026#39; AND `status` = 0; # 4、提交事务 commit; # 5、返回券码 `c1` 假设操作顺序如下：\n操作顺序 线程一 线程二 1 begin; 2 begin; 3 SELECT * FROM coupon WHERE status = 0 LIMIT 1 FOR UPDATE SKIP LOCKED; # 假设返回记录券码为 c1 4 SELECT * FROM coupon WHERE status = 0 LIMIT 1 FOR UPDATE SKIP LOCKED; # 成功，返回券码 c2 5 UPDATE coupon SET status = 1, user_id = 1000 WHERE code = \u0026lsquo;c1\u0026rsquo; AND status = 0; # 成功 6 commit; # 释放记录c1的锁 8 UPDATE coupon SET status = 1, user_id = 1000 WHERE code = \u0026lsquo;c2\u0026rsquo; AND status = 0; # 成功 9 commit; # 释放记录c2的锁 不同于方法二，线程二在执行第 4 步时，不会阻塞在记录c1上，而是会跳过这条记录，直接对下一条记录c2进行加锁。\n这个方法，锁的持有时间与方法二相同，但避免了争抢锁，也能达到一个很好的性能。\n注意，SKIP LOCKED 是 MySQL 8.0.1 引入的新特性。\n方法五：使用 Redis 减少 SQL 的执行次数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 0、初始化 Redis: 在活动开始前，先从数据库中查询所有未发放券的券码，放入 Redis 中的队列中 LPUSH coupon_list_key (SELECT code from coupon WHERE status = 0) # 1、发放时，从 Redis 队列中移除并返回一个券码，假设值为`c1` LPOP coupon_list_key # 2、修改券记录的状态 UPDATE coupon SET `status` = 1, `user_id` = 1000 WHERE `code` = \u0026#39;c1\u0026#39; AND `status` = 0; # 3、返回券码 `c1` 使用 Redis，既能保证正确性，也有更好的性能，因为：\nRedis 是单线程执行命令的，因此从 Redis 返回的券码是唯一的，执行第 2 步不会争抢锁也不会失败； LPOP 命令是从队列头移除一个元素，时间复杂度仅为 O(1)； 仅需要执行一条 UPDATE 语句； 除此之外，还可以通过引入 MQ 将 UPDATE 语句改为异步执行，进一步提高并发度。\n不过，引入 Redis 会使得系统变得更复杂，因为：\n需要额外编写一个模块用来初始化 Redis； 在初始化 Redis 要特别小心，不能将『已发放』的券放入 Redis； 从 Redis 出队成功，但写数据库失败，会导致券码被浪费，需要回滚 Redis； 回滚 Redis 可能实现起来比较复杂，因为将券码放回 Redis 时可能失败，需要重试机； Redis 主从同步可能存在延迟，此时主从切换可能导致 Redis 返回重复的券码，导致方法失败，需要重试机制； 小节 方法一和方法二不适合高并发场景，前者失败率高，后者性能特别差； 方法三持有锁的时间很短，性能比较好，实现也很简单，但存在热点数据更新问题； 方法四持有锁的时间稍长，因为锁的是不同的记录，因此不存在热点数据，性能比较好，实现也简单，但只能在 MySQL 8.0.1 及以上才能使用； 方法五引入 Redis，在性能上有最大的潜力，但实现起来更复杂； 方法三、四、五孰优孰劣，留待实践进行检验。\nUpdate just one unused row 仅更新一行未使用的数据，是一种非常常见的场景，看似非常容易解决：\n1 2 3 4 5 # 乐观锁 CAS 更新 UPDATE record SET `status` = \u0026#39;used\u0026#39; WHERE `status` = \u0026#39;unused\u0026#39; LIMIT 1; 但当我们需要获取被更新数据的 ID 时，问题瞬间变得复杂起来。\n获取变更 ID，除了上面介绍的使用悲观锁SELECT FOR UPDATE和LAST_INSERT_ID两个方法外，还可以通过临时变量来实现：\nSET @update_id := 0; UPDATE some_table SET column_name = \u0026lsquo;value\u0026rsquo;, id = (SELECT @update_id := id) WHERE some_other_column = \u0026lsquo;blah\u0026rsquo; LIMIT 1; SELECT @update_id;\n详情见：How to get ID of the last updated row in MySQL?:\n这种方法在需要一次更新多条记录时特别有用。\nMySQL 8.0.1 SKIP LOCKED and NOWAIT 推荐一篇文章：MySQL 8.0.1: Using SKIP LOCKED and NOWAIT to handle hot rows 这篇文章是在 2017 年 4 月 12 由 Martin Hansson 发布在 MySQL 官方博客上的。它主要是通过一个订票系统，讲解 SKIP LOCKED 和 NOWAIT 的使用方法。\n另外，需要特别注意：\nStatements that use NOWAIT or SKIP LOCKED are unsafe for statement based replication.\nReference https://dba.stackexchange.com/questions/131051/update-just-one-unused-row https://stackoverflow.com/questions/1388025/how-to-get-id-of-the-last-updated-row-in-mysql https://gist.github.com/PieterScheffers/189cad9510d304118c33135965e9cddb https://dev.mysql.com/blog-archive/mysql-8-0-1-using-skip-locked-and-nowait-to-handle-hot-rows/ https://dev.mysql.com/doc/refman/8.0/en/information-functions.html#function_last-insert-id https://dev.mysql.com/doc/refman/8.0/en/innodb-locking.html ","date":"2022-11-20T17:21:39+08:00","permalink":"https://zhumengzhu.github.io/2022/11/mysql-update-just-one-unused-row/","title":"MySQL 中如何实现只更新未使用的一行数据"},{"content":"背景 这是一篇博客阅读笔记，原博客Late row lookups: InnoDB写于 2011 年，作者是一个叫 Quassnoi 的俄罗斯人。\nQuassnoi 在 2009 年时写了一篇文章讲 MySQL Limit 的性能优化(MySQL ORDER BY / LIMIT performance: late row lookups)，后来有读者提了两个问题：\nIs this workaround specific to MyISAM engine? How does PostgreSQL handle this? Quassnoi 写下这篇新博客，即为了回答上述两个问题。\n正文 The questions concerns a certain workaround for MySQL LIMIT … OFFSET queries like this:\n1 2 3 4 SELECT * FROM mytable ORDER BY id LIMIT 10 OFFSET 10000; which can be improved using a little rewrite:\n1 2 3 4 5 6 7 8 SELECT m.* FROM (SELECT id FROM mytable ORDER BY id LIMIT 10 OFFSET 10000) q JOIN mytable m ON m.id = q.id ORDER BY m.id; 注意：作者之前的文章讨论的是这个方法对 MyISAM 是有效的；问题是，对于 InnoDB 和 PostgreSQL 呢？\nPostgreSQL The Answer The second questions is easy: PostgreSQL won\u0026rsquo;t pull the fields from the table until it really needs them. If a query involving an ORDER BY along with LIMIT and OFFSET is optimized to use the index for the ORDER BY part, the table lookups won\u0026rsquo;t happen for the records skipped.\n这句话是说，PostgreSQL 只会在需要的时候才回表查询。如果一个查询涉及 ORDER BY、LIMIT 和 OFFSET，那么可以先利用索引跳过不需要的记录，只对需要的记录进行进行回表。\n这其实就是『Late Row Lookups』；与之相对的，MySQL 执行的是『Early Row Lookups』。\n作者后面说虽然 PostgreSQL 的查询计划不会输出回表信息，但可以通过一个简单的测试进行验证。但具体怎么进行这个实验，作者没讲，下面试着做个补充。\n验证(存疑) 建表详情 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 -- 建表 CREATE TABLE page ( id SERIAL PRIMARY KEY, name VARCHAR(16) DEFAULT NULL, content VARCHAR(255) DEFAULT NULL ); -- 为 name 字段创建二级索引 CREATE INDEX idx_name ON page (name); -- 初始化 600 万条数据 INSERT INTO page (name, content) (SELECT CONCAT(\u0026#39;小瓦\u0026#39;, s.id) AS name, CONCAT(\u0026#39;xx\u0026#39;, s.id) AS content FROM GENERATE_SERIES(1, 6000010) AS s(id)); 执行 SQL 一：直接查询 1 2 3 4 5 6 7 postgres=# EXPLAIN ANALYZE SELECT * FROM page ORDER BY id OFFSET 6000000 LIMIT 10; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------------------ Limit (cost=199830.43..199830.76 rows=10 width=26) (actual time=540.002..540.003 rows=10 loops=1) -\u0026gt; Index Scan using page_pkey on page (cost=0.43..199846.81 rows=6000492 width=26) (actual time=0.087..427.982 rows=6000010 loops=1) Planning Time: 0.108 ms Execution Time: 540.022 ms 执行 SQL 二：使用子查询 1 2 3 4 5 6 7 8 9 10 11 postgres=# EXPLAIN ANALYZE SELECT t1.* FROM page t1, (SELECT id FROM page ORDER BY id OFFSET 6000000 LIMIT 10) t2 where t1.id=t2.id ORDER BY t1.id; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------------------------------------- Nested Loop (cost=155811.47..155895.90 rows=10 width=26) (actual time=368.952..368.960 rows=10 loops=1) -\u0026gt; Limit (cost=155811.04..155811.30 rows=10 width=4) (actual time=368.941..368.942 rows=10 loops=1) -\u0026gt; Index Only Scan using page_pkey on page (cost=0.43..155823.81 rows=6000492 width=4) (actual time=0.015..248.584 rows=6000010 loops=1) Heap Fetches: 50 -\u0026gt; Index Scan using page_pkey on page t1 (cost=0.43..8.45 rows=1 width=26) (actual time=0.001..0.001 rows=1 loops=10) Index Cond: (id = page.id) Planning Time: 0.375 ms Execution Time: 369.004 ms 执行 SQL 三：使用子查询 1 2 3 4 5 6 7 8 9 10 11 postgres=# EXPLAIN ANALYZE SELECT t1.* FROM page t1 join (SELECT id FROM page ORDER BY id OFFSET 6000000 LIMIT 10) t2 ON t1.id=t2.id ORDER BY t1.id; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------------------------------------- Nested Loop (cost=155811.47..155895.90 rows=10 width=26) (actual time=362.645..362.652 rows=10 loops=1) -\u0026gt; Limit (cost=155811.04..155811.30 rows=10 width=4) (actual time=362.633..362.634 rows=10 loops=1) -\u0026gt; Index Only Scan using page_pkey on page (cost=0.43..155823.81 rows=6000492 width=4) (actual time=0.017..243.148 rows=6000010 loops=1) Heap Fetches: 50 -\u0026gt; Index Scan using page_pkey on page t1 (cost=0.43..8.45 rows=1 width=26) (actual time=0.001..0.001 rows=1 loops=10) Index Cond: (id = page.id) Planning Time: 4.609 ms Execution Time: 362.707 ms 三个查询的耗时为：`540.022 ms` vs `369.004 ms` vs `362.707 ms`。 \u003e Tips: 使用 `EXPLAIN ANALYZE` 既可以获得查询计划，又能执行语句。 结论 上述实验可以证明该优化对于 PostgreSQL 同样可以有效。\nInnoDB 为了回答这个问题，作者首先创建了一张表。\n建表 建表详情 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # 创建一张内存表 CREATE TABLE filler ( id INT NOT NULL PRIMARY KEY AUTO_INCREMENT ) ENGINE = Memory; # 创建 InnoDB 表 CREATE TABLE lookup ( id INT NOT NULL PRIMARY KEY, value INT NOT NULL, shorttxt TEXT NOT NULL, longtxt TEXT NOT NULL ) ENGINE = InnoDB ROW_FORMAT = COMPACT; # 为 value 字段创建索引 CREATE INDEX ix_lookup_value ON lookup (value); # 创建存储计划 DELIMITER $$ CREATE PROCEDURE prc_filler(cnt INT) BEGIN DECLARE _cnt INT; SET _cnt = 1; WHILE _cnt \u0026lt;= cnt DO INSERT INTO filler SELECT _cnt; SET _cnt = _cnt + 1; END WHILE; END $$ # 初始化内存表 DELIMITER ; START TRANSACTION; CALL prc_filler(100000); COMMIT; # 初始化 InnoDB 表 INSERT INTO lookup SELECT id, CEILING(RAND(20110211) * 1000000), RPAD(\u0026#39;\u0026#39;, CEILING(RAND(20110211 \u0026lt;\u0026lt; 1) * 100), \u0026#39;*\u0026#39;), RPAD(\u0026#39;\u0026#39;, CEILING(8192 + RAND(20110211 \u0026lt;\u0026lt; 1) * 100), \u0026#39;*\u0026#39;) FROM filler; 上面利用一张内存表和存储计划创建了一张 InnoDB 表 `lookup`：这张表包含一个加了索引的 `INT` 列，以及两个 `TEXT` 列，其中 shorttxt 存储短字符串(包含 1~100 个字符)，longtxt 存储长字符串(包含 8193~8293 个字符)。 通过主键索引查询 value 和 shottxt 两个字段时，是否使用子查询优化对耗时影响不大，略去不讨论。\n通过主键索引查询 longtxt 列 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Rewrite SELECT LENGTH(l.longtxt) FROM (SELECT id FROM lookup ORDER BY id LIMIT 10 OFFSET 90000) q JOIN lookup l ON l.id = q.id ORDER BY q.id; # 10 rows in set (0.133 sec) # No rewrite SELECT LENGTH(longtxt) FROM lookup ORDER BY id LIMIT 10 OFFSET 90000; # 10 rows in set (1.579 sec) 0.133 sec vs 1.579 sec。\nWhy such a difference?\nThe reason is that InnoDB, despite the fact it stores the data in the clustered index, is still able to move \u0026gt; some data out of the index. This is called external storage.\n在 InnoDB 中，小于 768 字节的列会全部存储在页上，大于 768 字节则会分开存储。在上面的 lookup 表中，shottxt 列总是 on-page 存储的，而 longtxt 则是 off-page的。\n因此，直接查询 longtxt 时，每扫描一条记录都要出发两次page lookups：第一次查聚簇索引，第二次查外部存储。这既会花费很多时间，也可能破坏 InnoDB 缓存，导致缓存命中率下降。\n通过二级索引查询 shorttxt 列 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Rewrite SELECT LENGTH(l.shorttxt) FROM (SELECT id, value FROM lookup ORDER BY value LIMIT 10 OFFSET 90000) q JOIN lookup l ON l.id = q.id ORDER BY q.value; # 10 rows in set (0.022 sec) # No rewrite SELECT LENGTH(shorttxt) FROM lookup ORDER BY value LIMIT 10 OFFSET 90000;# 10 rows in set (0.209 sec) 0.022 sec vs 0.209 sec，耗时相差 10 倍。 InnoDB的二级索引查询有点类似MyISAM，都需要一次额外的回表查询去获取正在的数据。\n上面第一个查询对于跳过的记录不会执行回表查询，因此速度是原来的 10 倍。它甚至比在主键索引上的查询(0.044 sec)还快，原因是二级索引包含的数据比主键索引更少，每页保存的数据更多，因此索引可能更加矮胖，扫描速度更快。\n当然，二级索引上同样是Early Row Lookups。\n结论 A trick used to avoid early row lookups for the LIMIT … OFFSET queries is useful on InnoDB tables too, though to different extent, depending on the ORDER BY condition and the columns involved:\nIt\u0026rsquo;s very useful on queries involving columns stored off-page (long TEXT, BLOB and VARCHAR columns) It\u0026rsquo;s very useful on ORDER BY conditions served by secondary indexes It\u0026rsquo;s quite useful on moderate sized columns (still stored on page) or CPU-intensive expressions It\u0026rsquo;s almost useless on short columns without complex CPU-intensive processing 其它 PostgreSQL查询计划 除第一行以外每个-\u0026gt;表示一个子动作 查询计划的阅读顺序都是从后往前 cost 由 .. 分割成两个数字，第一个数字表示启动成本，即返回第一行的成本；第二个数字表示返回所有数据的成本。 rows 表示返回行数 width 表示每行平均宽度 loops 表示索引扫描被执行过几次 Reference Documentation: 15: 14.1. Using EXPLAIN - PostgreSQL ","date":"2022-11-14T19:12:47+08:00","permalink":"https://zhumengzhu.github.io/2022/11/late-row-lookups-innodb/","title":"Late Row Lookups: InnoDB"},{"content":"一、深分页为什么慢 深分页指的是形如 select ... from ... where ... order by ... limit offset, size 的查询语句中 offset 特别大的情况。高性能 MySQL(第三版) 的第 6.7.5 节专门讲了此问题，但比较简略。\n有多慢 假设有一张 page 表包含 600 多万条数据，分别执行下面两条语句：\n1 2 3 4 5 6 7 # SQL 一 select * from page order by id limit 0, 10; # 10 rows in set (0.001 sec) # SQL 二 select * from page order by id limit 6000000, 10; # 10 rows in set (0.940 sec) 注意到 SQL 二其实是按主键 ID 排序的(意味着不需要进行 filesort，直接按主键索引顺序扫描即可)，耗时仍然高达 0.940 秒。\n为什么慢 『查询计划』\n1 2 3 4 5 6 explain select * from page order by id limit 6000000, 10; +------+-------------+-------+-------+---------------+---------+---------+------+---------+-------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +------+-------------+-------+-------+---------------+---------+---------+------+---------+-------+ | 1 | SIMPLE | page | index | NULL | PRIMARY | 4 | NULL | 5988448 | | +------+-------------+-------+-------+---------------+---------+---------+------+---------+-------+ 『慢查询日志』\n1 2 3 4 5 6 7 # Time: 221106 0:26:54 # User@Host: root[root] @ localhost [] # Thread_id: 18 Schema: test QC_hit: No # Query_time: 0.939618 Lock_time: 0.000735 Rows_sent: 10 Rows_examined: 6000010 # Rows_affected: 0 Bytes_sent: 533 SET timestamp=1667665614; select * from page order by id limit 6000000, 10; 注意到 rows 和 为 rows_examined 分别为 5988448、6000010。 原因显而易见：为了完成 limit offset, size 这样的查询， MySQL 要扫描至少 offset + size 行数据；offset 越大，则扫描次数越多，速度越慢。\n二、深分页怎么优化 分页从原理上来讲，主要是两种：\n一种就是前面讲的基于 limit offset, size 的分页，英文一般叫 offset pagination； 另一种方法放弃了 offset，改用 left off, SQL 形如 SELECT ... WHERE ... AND id \u0026lt; $left_off ORDER BY id DESC LIMIT 10, 英文一般叫 cursor pagination, 也有人称之为 seek method 或 keyset pagination。 Offset pagination 就像基于比较的排序算法的平均时间复杂度的下界是 O(nlogn) 一样，对于 MySQL 来说，凡是形如 limit offset, size 的查询，扫描次数的下限就是 offset + size，我们只能尽量逼近这个下限。 由于在主键索引上的优化比较复杂，同样的 SQL 对于不同的数据规模效果不一样，暂不讨论。这里主要分析在二级索引上的优化。\n对于拥有 600 多万条数据的 page 表：\n1 2 3 4 5 6 7 8 9 10 -- 原 SQL(强制走二级索引) select * from page force index(idx_name) order by name limit 6000000, 10; # 10 rows in set (4.833 sec) -- 优化方案一 select t1.* from page t1, (select id from page order by name limit 6000000, 10) t2 where t1.id = t2.id order by t1.name; # 10 rows in set (0.746 sec) -- 优化方案二 select t1.* from page t1 join (select id from page order by name limit 6000000, 10) t2 on t1.id = t2.id order by t1.name; # 10 rows in set (0.741 sec) 通过使用利用了覆盖索引的子查询，性能提升约 80%~90%：\nKeyset pagination 1 2 3 4 # First page (latest 10 items): SELECT ... WHERE ... ORDER BY id DESC LIMIT 10 # Next page (second 10): SELECT ... WHERE ... AND id \u0026lt; $left_off ORDER BY id DESC LIMIT 10 三、优化为什么有效 优化前 考虑原始 SQL select * from page force index(idx_name) order by name limit 6000000, 10 的执行过程：\n首先 Sever 层向 InnoDB 请求第一条数据；InnoDB 从 idx_name 上获取第一条二级索引记录，然后查询聚簇索引获取完整记录(即回表操作)，返回给 Server 层; 由于存在 limit 6000000 的限制, Server 层会将该数据丢弃并计数，然后向 InnoDB 请求下一条数据； 上述操作重复 600 万次； 之后的第 6000001~6000010 10 条数据则会被放入本地网络缓冲区，发给客户端； 问题在第 1 步的回表操作：\n回表首先意味着先读一次二级索引，然后读一次聚簇索引，因此记录的扫描总数实际会是 2 * 600 万次=1200 万次； 其次，回表操作可能需要一次磁盘随机读； 优化后 再考虑优化后的 SQL select t1.* from page t1 join (select id from page order by name limit 6000000, 10) t2 on t1.id = t2.id order by t1.name 的执行过程：\n首先 Sever 层向 InnoDB 请求第一条数据；InnoDB 从 idx_name 上获取第一条二级索引记录，然后查询聚簇索引获取完整记录(这一操作叫回表)，然后将主键 ID 返回给 Server 层; 由于存在 limit 6000000 的限制, Server 层会将该数据丢弃并计数，然后向 InnoDB 请求下一条数据； 上述操作重复 600 万次； 之后的第 6000001~6000010 10 条数据则会被放入内存缓冲区； 查询聚簇索引，获取 10 个主键 ID 对应的完整记录； 总结 在优化后：\n记录的总扫描次数从 1200 万次 减少到 600 万零 10 次； 磁盘随机读的次数从 600 万次 减少到 10 次； 四、试验方法 建表和数据初始化 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 -- 建表 CREATE TABLE `page` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `name` VARCHAR(16) DEFAULT NULL, `content` VARCHAR(255) DEFAULT NULL, PRIMARY KEY (`id`), KEY `idx_name` (`name`) ) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4; -- 创建存储过程用于初始化数据 DELIMITER ;; CREATE PROCEDURE init_page() BEGIN DECLARE i INT; SET i = 1; WHILE(i \u0026lt;= 6000010) DO INSERT INTO page (`name`, `content`) VALUES (CONCAT(\u0026#39;小瓦\u0026#39;, i), CONCAT(\u0026#39;xx\u0026#39;, i)); SET i = i + 1; END WHILE; END;; DELIMITER ; -- 调用存储过程 CALL init_page(); 启用慢查询日志 1 2 3 4 5 6 -- 启用慢查询日志 SET GLOBAL slow_query_log=1; -- 将慢查询时间阈值设置为 0.1 秒 SET GLOBAL long_query_time=0.1; -- 查看慢查询日志名 SHOW VARIABLES LIKE \u0026#39;%slow_query%\u0026#39;; 对比 SQL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 -- 探索一级索引 --- 基础 SQL select * from page order by id limit 6000000, 10; --- 最慢 select * from page where id \u0026gt;= (select id from page order by id limit 6000000, 1) order by id limit 10; --- 最快 select t1.* from page t1, (select id from page order by id limit 6000000, 10) t2 where t1.id = t2.id order by t1.id; -- 探索二级索引 --- 基础 SQL，会是全表扫描，最慢 select * from page order by name limit 6000000, 10; --- 强制走二级索引，稍快 select * from page force index(idx_name) order by name limit 6000000, 10; --- 利用使用了覆盖索引的子查询，减少回表，最快 select t1.* from page t1, (select id from page order by name limit 6000000, 10) t2 where t1.id = t2.id order by t1.name; 查看慢查询日志和数据存储位置 1 2 3 4 5 6 # 查看 mysql 进程参数 ps aux | grep mysql # /opt/homebrew/opt/mariadb/bin/mariadbd --basedir=/opt/homebrew/opt/mariadb --datadir=/opt/homebrew/var/mysql --plugin-dir=/opt/homebrew/opt/mariadb/lib/plugin --log-error=/opt/homebrew/var/mysql/zmz.local.err --pid-file=zmz.local.pid # --basedir=/opt/homebrew/opt/mariadb 即慢查询日志和数据文件所在目录 # ps. 执行前面的存储计划，创建 100 万条数据，占用磁盘空间大约 100MB；创建 600 万条数据，占用空间大约 600MB； 五、References mysql查询 limit 1000,10 和limit 10 速度一样快吗？如果我要分页，我该怎么办？ 要想通过面试，MySQL的Limit子句底层原理你不可不知 MySQL Logical Architecture Mysql index configuration MySQL ORDER BY / LIMIT performance: late row lookups Late Row Lookups: InnoDB MySQL ORDER BY LIMIT Performance Optimization mysql 证明为什么用limit时，offset很大会影响性能 MySQL 5.7 Reference Manual/LIMIT Query Optimization Pagination Optimization We need tool support for keyset pagination ","date":"2022-11-05T12:36:01+08:00","permalink":"https://zhumengzhu.github.io/2022/11/mysql-order-by-limit-performance-optimization/","title":"Mysql 的深分页问题及优化方法"},{"content":"常用命令 hugo new site SITE_NAME 生成静态博客项目 hugo server 启动本地服务器，加上 -D 可以渲染 draft=true 的文章 hugo new post/new-content.md 在 post 下新建一篇文章 hugo 生成站点静态文件(public 和 resources 目录) hugo list drafts/expired/future 列出草稿/过期/未来的文件 踩坑记 使用主题 Event 时, 文章必须放在 content/**post**/ 目录下, 否则 Home 页不会展示文章链接, 文章内也不会展示目录 使用主题 Zzo 时, 要创建 Archive 页(类似 Event 中的 Home 页), 需要创建文件 content/archive/_index.md(参考 How to automatically generate archive page content #47) 使用主题 Zzo 时, 在 GigHub Actions 配置中必须启用 Hugo extended 模式, 否则构建会失败(参考Hugo setup) 参考链接 Hugo 官方文档 Hugo 从入门到会用 Hugo 搭建博客实践 Hugo+Stack 博客修改记录 hugo升级与hugo-theme-stack主题修改与最后修改时间问题 ","date":"2022-10-30T22:22:52+08:00","permalink":"https://zhumengzhu.github.io/2022/10/hugo-quick-start/","title":"Hugo 配置记录"},{"content":"Mac 客户端 推荐 V2rayU Terminal 代理 1 2 # .config/fish/config.fish alias all_proxy=\u0026#39;export http_proxy=http://127.0.0.1:1087;export https_proxy=http://127.0.0.1:1087;export ALL_PROXY=socks5://127.0.0.1:1080\u0026#39; git ssh 代理 1 2 3 4 # .ssh/config Host github.com User git ProxyCommand nc -v -x 127.0.0.1:1080 %h %p ","date":"2022-10-16T19:09:32+08:00","permalink":"https://zhumengzhu.github.io/2022/10/how-to-set-network-proxy/","title":"代理设置"},{"content":"PromQL 参考：https://blog.csdn.net/han949417140/article/details/113513227\n官网：https://prometheus.io/docs/prometheus/latest/querying/basics/\nhttps://grafana.com/docs/grafana/v7.5/panels/queries/ https://prometheus.fuckcloudnative.io/di-san-zhang-prometheus/di-4-jie-cha-xun/basics https://github.com/google/re2/wiki/Syntax 基本概念 - 时间序列(time-series) - 样本 - time-series 是按照时间戳和值的序列顺序存放的，我们称之为向量(vector) - 每条 time-series 通过指标名称(metrics name)和一组标签集(labelset)命名 - 在time-series中的每一个点称为一个样本（sample），样本由以下三部分组成： - 指标(metric)：metric name和描述当前样本特征的labelsets; - 时间戳(timestamp)：一个精确到毫秒的时间戳; - 样本值(value)： 一个float64的浮点型数据表示当前样本的值。 - 指标(Metric) - 在形式上，所有的指标(Metric)都通过如下格式标示： - \u0026lt;metric name\u0026gt;{\u0026lt;label name\u0026gt;=\u0026lt;label value\u0026gt;, ...} - 指标的名称(metric name)可以反映被监控样本的含义（比如，http_request_total - 表示当前系统接收到的HTTP请求总量） - 标签(label)反映了当前样本的特征维度，通过这些维度Prometheus可以对样本数据进行过滤，聚合等 - 以__作为前缀的标签，是系统保留的关键字，只能在系统内部使用 - Metrics类型 - Prometheus定义了4种不同的指标类型(metric type)：Counter（计数器）、Gauge（仪表盘）、Histogram（直方图）、Summary（摘要） - Counter：只增不减的计数器 - Counter类型的指标其工作方式和计数器一样，只增不减（除非系统发生重置）。 - 常见的监控指标，如http_requests_total，node_cpu都是Counter类型的监控指标。 一般在定义Counter类型指标的名称时推荐使用_total作为后缀。 - //例如，通过rate()函数获取HTTP请求量的增长率： rate(http_requests_total[5m]) - //查询当前系统中，访问量前10的HTTP地址： topk(10, http_requests_total) - Gauge：可增可减的仪表盘 - 与Counter不同，Gauge类型的指标侧重于反应系统的当前状态。因此这类指标的样本数据可增可减。 - 常见指标如：node_memory_MemFree（主机当前空闲的内容大小）、node_memory_MemAvailable（可用内存大小）都是Gauge类型的监控指标。 - //通过Gauge指标，用户可以直接查看系统的当前状态：node_memory_MemFree - //还可以使用deriv()计算样本的线性回归模型，甚至是直接使用predict_linear()对数据的变化趋势进行预测。 - 例如，预测系统磁盘空间在4个小时之后的剩余情况：predict_linear(node_filesystem_free{job=\u0026quot;node\u0026quot;}[1h], 4 * 3600) - Histogram和Summary分析数据分布情况 Instant vector vs Range vector https://stackoverflow.com/questions/68223824/prometheus-instant-vector-vs-range-vector https://satyanash.net/software/2021/01/04/understanding-prometheus-range-vectors.html https://promlabs.com/blog/2020/06/18/the-anatomy-of-a-promql-query/ Rate then sum, never sum then rate https://www.robustperception.io/rate-then-sum-never-sum-then-rate/ To help keep you on the straight and narrow, remember this:\nThe only mathematical operations you can safely directly apply to a counter\u0026rsquo;s values are rate, irate, increase, and resets. Anything else will cause you problems.\nThe problem of rate()/increase() extrapolation in Prometheus https://stackoverflow.com/questions/52697517/prometheus-how-to-rate-a-sum-of-the-same-counter-from-different-machines?rq=3 https://github.com/prometheus/prometheus/issues/3746 https://github.com/prometheus/prometheus/issues/3806 https://github.com/VictoriaMetrics/VictoriaMetrics/issues/1215 https://docs.google.com/document/d/1y2Mp041_2v0blnKnZk7keCnJZICeK2YWUQuXH_m4DVc/edit?pli=1\u0026tab=t.0#heading=h.bupciudrwmna https://lotabout.me/2019/QQA-Why-Prometheus-increase-return-float/ (中文带图) Aggregating the precomputed quantiles from a summary makes no sense https://prometheus.io/docs/practices/histograms/#count-and-sum-of-observations avg(http_request_duration_seconds{quantile=\u0026ldquo;0.95\u0026rdquo;}) // BAD! histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le)) // GOOD.\nErrors of quantile estimation https://prometheus.io/docs/practices/histograms/#errors-of-quantile-estimation Quantiles, whether calculated client-side or server-side, are estimated. It is important to understand the errors of that estimation.\nContinuing the histogram example from above, imagine your usual request durations are almost all very close to 220ms, or in other words, if you could plot the \u0026ldquo;true\u0026rdquo; histogram, you would see a very sharp spike at 220ms. In the Prometheus histogram metric as configured above, almost all observations, and therefore also the 95th percentile, will fall into the bucket labeled {le=\u0026ldquo;0.3\u0026rdquo;}, i.e. the bucket from 200ms to 300ms. The histogram implementation guarantees that the true 95th percentile is somewhere between 200ms and 300ms. To return a single value (rather than an interval), it applies linear interpolation, which yields 295ms in this case. The calculated quantile gives you the impression that you are close to breaching the SLO, but in reality, the 95th percentile is a tiny bit above 220ms, a quite comfortable distance to your SLO.\nNext step in our thought experiment: A change in backend routing adds a fixed amount of 100ms to all request durations. Now the request duration has its sharp spike at 320ms and almost all observations will fall into the bucket from 300ms to 450ms. The 95th percentile is calculated to be 442.5ms, although the correct value is close to 320ms. While you are only a tiny bit outside of your SLO, the calculated 95th quantile looks much worse.\nBest practices https://grafana.com/docs/grafana/v7.5/best-practices/\nQueries Examples https://prometheus.io/docs/prometheus/2.39/querying/examples/\nPrometheus 的各种问题 http://www.xuyasong.com/?p=1921 https://aleiwu.com/post/prometheus-bp/ https://develotters.com/posts/high-cardinality/ 高基数问题(High-Cardinality Problem) 2.16 版本以上的 Prometheus 直接支持查看 TSDB 的状态：\nCounter 重置导致的问题 Counter 是四种 Metrics 类型中的一种，它的值只会增加(incr)或者重置(reset, 一般是因为实例重启)。\nPromQL 的 rate()和increase() 函数会自动处理 Counter 重置的情况，但 sum 不会，所以很多文档都强调一定要先 rate 再 sum，而不能先 sum 再 rate。\nrate/increase 是如何处理 counter 重置的？ 具体的处理代码为：\n1 2 3 4 5 6 7 8 9 // Handle counter resets: resultFloat = samples.Floats[numSamplesMinusOne].F - samples.Floats[0].F prevValue := samples.Floats[0].F for _, currPoint := range samples.Floats[1:] { if currPoint.F \u0026lt; prevValue { resultFloat += prevValue } prevValue = currPoint.F } 首先遍历样本，如果发现 Counter 减少就认为发生了重置，后续所有样本值都会加上重置前的值。\n举个栗子，假设时间序列的值为[5,10,4,6]，那么它会被处理为[5,10,14,16]。\nPrometheus 报警不准问题 https://aleiwu.com/post/prometheus-alert-why/ 报警不准，指的是两类问题： 『该报』的警没报； 『不该报』的警报了； 这里的『该报』和『不该报』都是基于 Grafana 进行判断的。所以这里说报警不准，本质是 Grafana 的仪表盘展示结果和 Prometheus 告警不一致。 这个问题本质是『采样间隔』不一致的：\nPrometheus 的告警器会按固定的时间间隔采样，在一个周期内，如果样本满足告警规则； Grafana 渲染图标的结果，则取决于基于 Range Query 采样点的分布； 很可能出现，Prometheus 采样时捕获了『低谷』但 Grafana 忽略了『低谷』导致『不该报』的警报了，或者 Prometheus 忽略了『低谷』而 Grafana 捕获了『低谷』导致『该报』的警没报。\nPrometheus: Alertmanager 架构图 为什么要 Alertmanager？ 当 Prometheus Server 计算出一些警报后，它自己并没有能力将这些警报通知出去，只能将警报推给 Alertmanager，由 Alertmanager 进行发送。\n这个切分，一方面是出于单一职责的考虑，让 Prometheus “do one thing and do it well”, 另一方面则是因为警报发送确实不是一件”简单”的事，需要一个专门的系统来做好它。\n可以这么说，Alertmanager 的目标不是简单地”发出警报”，而是”发出高质量的警报”。它提供的高级功能包括但不限于：\nGo Template 渲染警报内容； 管理警报的重复提醒时机与消除后消除通知的发送； 根据标签定义警报路由，实现警报的优先级、接收人划分，并针对不同的优先级和接收人定制不同的发送策略； 将同类型警报打包成一条通知发送出去，降低警报通知的频率； 支持静默规则: 用户可以定义一条静默规则，在一段时间内停止发送部分特定的警报，比如已经确认是搜索集群问题，在修复搜索集群时，先静默掉搜索集群相关警报； 支持”抑制”规则(Inhibition Rule): 用户可以定义一条”抑制”规则，规定在某种警报发生时，不发送另一种警报，比如在”A 机房网络故障”这条警报发生时，不发送所有”A 机房中的警报”； Routing Tree Routing Tree 的设计意图是让用户能够非常自由地给警报归类，然后根据归类后的类别来配置要发送给谁以及怎么发送：\n发送给谁？上面已经做了很好的示例，’数据库警报’和’前端警报’都有特定的接收组，都没有匹配上那么就是’默认警报’, 发送给默认接收组 怎么发送？对于一类警报，有个多个字段来配置发送行为： group_by：决定了警报怎么分组，每个 group 只会定时产生一次通知，这就达到了降噪的效果，而不同的警报类别分组方式显然是不一样的，举个例子： 配置中的 ‘数据库警报’ 是按 ‘集群’ 和 ‘规则名’ 分组的，这表明对于数据库警报，我们关心的是“哪个集群的哪个规则出问题了”，比如一个时间段内，’华东’集群产生了10条 ‘API响应时间过长’ 警报，- 这些警报就会聚合在一个通知里发出来； 配置中的 ‘前端警报’ 是按 ‘产品’ 和 ‘环境’ 分组的， 这表明对于前端警报，我们关心的是“哪个产品的哪个环境出问题了” group_interval 和 group_wait: 控制分组的细节，不细谈，其中 group_interval 控制了这个分组最快多久执行一次 Notification Pipeline repeat_interval: 假如一个相同的警报一直 FIRING，Alertmanager 并不会一直发送警报，而会等待一段时间，这个等待时间就是 repeat_interval，显然，不同类型警报的发送频率也是不一样的 Notification Pipeline 要重点说的是DedupStage和NotifySetStage它俩协同负责去重工作，具体做法是：\nNotifySetStage 会为发送成功的警报记录一条发送通知，key 是’接收组名字’+’GroupKey 的 key 值’，value 是当前 Stage 收到的 []Alert (这个列表和最开始进入 Notification Pipeline 的警报列表有可能是不同的，因为其中有些 Alert 可能在前置 Stage 中已经被过滤掉了) DedupStage 中会以’接收组名字’+’GroupKey 的 key 值’为 key 查询通知记录，假如： 查询无结果，那么这条通知没发过，为这组警报发送一条通知； 查询有结果，那么查询得到已经发送过的一组警报 S，判断当前的这组警报 A 是否为 S 的子集： 假如 A 是 S 的子集，那么表明 A 和 S 重复，这时候要根据 repeat_interval 来决定是否再次发送： 距离 S 的发送时间已经过去了足够久（repeat_interval)，那么我们要再发送一遍； 距离 S 的发送时间还没有达到 repeat_interval，那么为了降低警报频率，触发去重逻辑，这次我们就不发了； 假如 A 不是 S 的子集，那么 A 和 S 不重复，需要再发送一次； 上面的表述可能有些抽象，最后表现出来的结果是： 假如一个 AlertGroup 里的警报一直发生变化，那么虽然每次都是新警报，不会被去重，但是由于 group_interval （假设是5分钟）存在，这个 AlertGroup 最多 5 分钟触发一次 Notification Pipeline，因此最多也只会 5 分钟发送一条通知； 假如一个 AlertGroup 里的警报一直不变化，就是那么几条一直 FIRING 着，那么虽然每个 group_interval 都会触发 Notification Pipeline，但是由于 repeate_interval（假设是1小时）存在，因此最多也只会每 1 小时为这个重复的警报发送一条通知； 再说一下 Silence 和 Inhibit，两者都是基于用户主动定义的规则的： Silence Rule：静默规则用来关闭掉部分警报的通知，比如某个性能问题已经修复了，但需要排期上线，那么在上线前就可以把对应的警报静默掉来减少噪音； Inhibit Rule：抑制规则用于在某类警报发生时，抑制掉另一类警报，比如某个机房宕机了，那么会影响所有上层服务，产生级联的警报洪流，反而会掩盖掉根本原因，这时候抑制规则就有用了； 因此 Notification Pipeline 的设计意图就很明确了：通过一系列逻辑（如抑制、静默、去重）来获得更高的警报质量，由于警报质量的维度很多（剔除重复、类似的警报，静默暂时无用的警报，抑制级联警报），因此 Notification Pipeline 设计成了责任链模式，以便于随时添加新的环节来优化警报质量 Prometheus存储机制 http://www.xuyasong.com/?p=1601 深入 PromQL Bascis https://stackoverflow.com/questions/68223824/prometheus-instant-vector-vs-range-vector https://satyanash.net/software/2021/01/04/understanding-prometheus-range-vectors.html https://promlabs.com/blog/2020/06/18/the-anatomy-of-a-promql-query/ Prometheus Querying - Breaking Down PromQL Functions Rate \u0026amp; Increase https://promlabs.com/blog/2021/01/29/how-exactly-does-promql-calculate-rates/ https://stackoverflow.com/questions/54494394/do-i-understand-prometheuss-rate-vs-increase-functions-correctly?rq=3 https://stackoverflow.com/questions/48218950/increase-in-prometheus-sometimes-doubles-values-how-to-avoid?rq=3 https://stackoverflow.com/questions/52697517/prometheus-how-to-rate-a-sum-of-the-same-counter-from-different-machines https://github.com/VictoriaMetrics/VictoriaMetrics/issues/1215 https://stackoverflow.com/questions/70835778/understanding-increase-and-rate-used-on-http-server-requests-seconds-count-w 由于 prometheus 使用了线性插值算法取计算 increase(增量)，所以计算结果会得到小数，这一点特别容易让人误解。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 // === rate(node parser.ValueTypeMatrix) Vector === func funcRate(vals []parser.Value, args parser.Expressions, enh *EvalNodeHelper) Vector { return extrapolatedRate(vals, args, enh, true, true) } // === increase(node parser.ValueTypeMatrix) Vector === func funcIncrease(vals []parser.Value, args parser.Expressions, enh *EvalNodeHelper) Vector { return extrapolatedRate(vals, args, enh, true, false) } // extrapolatedRate is a utility function for rate/increase/delta. // It calculates the rate (allowing for counter resets if isCounter is true), // extrapolates if the first/last sample is close to the boundary, and returns // the result as either per-second (if isRate is true) or overall. func extrapolatedRate(vals []parser.Value, args parser.Expressions, enh *EvalNodeHelper, isCounter, isRate bool) Vector { ms := args[0].(*parser.MatrixSelector) vs := ms.VectorSelector.(*parser.VectorSelector) var ( samples = vals[0].(Matrix)[0] rangeStart = enh.Ts - durationMilliseconds(ms.Range+vs.Offset) rangeEnd = enh.Ts - durationMilliseconds(vs.Offset) resultFloat float64 resultHistogram *histogram.FloatHistogram firstT, lastT int64 numSamplesMinusOne int ) // We need either at least two Histograms and no Floats, or at least two // Floats and no Histograms to calculate a rate. Otherwise, drop this // Vector element. if len(samples.Histograms) \u0026gt; 0 \u0026amp;\u0026amp; len(samples.Floats) \u0026gt; 0 { // Mix of histograms and floats. TODO(beorn7): Communicate this failure reason. return enh.Out } switch { case len(samples.Histograms) \u0026gt; 1: numSamplesMinusOne = len(samples.Histograms) - 1 firstT = samples.Histograms[0].T lastT = samples.Histograms[numSamplesMinusOne].T resultHistogram = histogramRate(samples.Histograms, isCounter) if resultHistogram == nil { // The histograms are not compatible with each other. // TODO(beorn7): Communicate this failure reason. return enh.Out } case len(samples.Floats) \u0026gt; 1: numSamplesMinusOne = len(samples.Floats) - 1 firstT = samples.Floats[0].T lastT = samples.Floats[numSamplesMinusOne].T resultFloat = samples.Floats[numSamplesMinusOne].F - samples.Floats[0].F if !isCounter { break } // Handle counter resets: prevValue := samples.Floats[0].F for _, currPoint := range samples.Floats[1:] { if currPoint.F \u0026lt; prevValue { resultFloat += prevValue } prevValue = currPoint.F } default: // Not enough samples. TODO(beorn7): Communicate this failure reason. return enh.Out } // Duration between first/last samples and boundary of range. durationToStart := float64(firstT-rangeStart) / 1000 durationToEnd := float64(rangeEnd-lastT) / 1000 sampledInterval := float64(lastT-firstT) / 1000 averageDurationBetweenSamples := sampledInterval / float64(numSamplesMinusOne) // TODO(beorn7): Do this for histograms, too. if isCounter \u0026amp;\u0026amp; resultFloat \u0026gt; 0 \u0026amp;\u0026amp; len(samples.Floats) \u0026gt; 0 \u0026amp;\u0026amp; samples.Floats[0].F \u0026gt;= 0 { // Counters cannot be negative. If we have any slope at all // (i.e. resultFloat went up), we can extrapolate the zero point // of the counter. If the duration to the zero point is shorter // than the durationToStart, we take the zero point as the start // of the series, thereby avoiding extrapolation to negative // counter values. durationToZero := sampledInterval * (samples.Floats[0].F / resultFloat) if durationToZero \u0026lt; durationToStart { durationToStart = durationToZero } } // If the first/last samples are close to the boundaries of the range, // extrapolate the result. This is as we expect that another sample // will exist given the spacing between samples we\u0026#39;ve seen thus far, // with an allowance for noise. extrapolationThreshold := averageDurationBetweenSamples * 1.1 extrapolateToInterval := sampledInterval if durationToStart \u0026lt; extrapolationThreshold { extrapolateToInterval += durationToStart } else { extrapolateToInterval += averageDurationBetweenSamples / 2 } if durationToEnd \u0026lt; extrapolationThreshold { extrapolateToInterval += durationToEnd } else { extrapolateToInterval += averageDurationBetweenSamples / 2 } factor := extrapolateToInterval / sampledInterval if isRate { factor /= ms.Range.Seconds() } if resultHistogram == nil { resultFloat *= factor } else { resultHistogram.Mul(factor) } return append(enh.Out, Sample{F: resultFloat, H: resultHistogram}) } histogramQuantitle 用法示例：\n1 TBD 源码见：\nhttps://github.com/prometheus/prometheus/blob/caa173d2aac4c390546b1f78302104b1ccae0878/promql/functions.go#L615 https://github.com/prometheus/prometheus/blob/caa173d2aac4c390546b1f78302104b1ccae0878/promql/quantile.go#L73 核心算法是：bucketStart + (bucketEnd-bucketStart)*(rank/count)。 同样也是线性插值算法，比较好理解。唯一需要注意的是，在 histogram_quantile 的第二个参数是 sum(rate, \u0026hellip;)，这个是 QPS(count/time) 而不是直接用的 count，为什么？计算公式里最后一项是 rank/count，而 rank = q*count，两者相除 time 项会被消除，相当于还是 count\u0026rsquo; / count，也就是结果是等价的。既然结果等价，为什么要用 rate 而不直接用 count？因为 rate 函数可以使用处理 reset，直接用 count 不行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 // === histogram_quantile(k parser.ValueTypeScalar, Vector parser.ValueTypeVector) Vector === func funcHistogramQuantile(vals []parser.Value, args parser.Expressions, enh *EvalNodeHelper) Vector { q := vals[0].(Vector)[0].F inVec := vals[1].(Vector) if enh.signatureToMetricWithBuckets == nil { enh.signatureToMetricWithBuckets = map[string]*metricWithBuckets{} } else { for _, v := range enh.signatureToMetricWithBuckets { v.buckets = v.buckets[:0] } } var histogramSamples []Sample for _, sample := range inVec { // We are only looking for conventional buckets here. Remember // the histograms for later treatment. if sample.H != nil { histogramSamples = append(histogramSamples, sample) continue } upperBound, err := strconv.ParseFloat( sample.Metric.Get(model.BucketLabel), 64, ) if err != nil { // Oops, no bucket label or malformed label value. Skip. // TODO(beorn7): Issue a warning somehow. continue } enh.lblBuf = sample.Metric.BytesWithoutLabels(enh.lblBuf, labels.BucketLabel) mb, ok := enh.signatureToMetricWithBuckets[string(enh.lblBuf)] if !ok { sample.Metric = labels.NewBuilder(sample.Metric). Del(excludedLabels...). Labels() mb = \u0026amp;metricWithBuckets{sample.Metric, nil} enh.signatureToMetricWithBuckets[string(enh.lblBuf)] = mb } mb.buckets = append(mb.buckets, bucket{upperBound, sample.F}) } // Now deal with the histograms. for _, sample := range histogramSamples { // We have to reconstruct the exact same signature as above for // a conventional histogram, just ignoring any le label. enh.lblBuf = sample.Metric.Bytes(enh.lblBuf) if mb, ok := enh.signatureToMetricWithBuckets[string(enh.lblBuf)]; ok \u0026amp;\u0026amp; len(mb.buckets) \u0026gt; 0 { // At this data point, we have conventional histogram // buckets and a native histogram with the same name and // labels. Do not evaluate anything. // TODO(beorn7): Issue a warning somehow. delete(enh.signatureToMetricWithBuckets, string(enh.lblBuf)) continue } enh.Out = append(enh.Out, Sample{ Metric: enh.DropMetricName(sample.Metric), F: histogramQuantile(q, sample.H), }) } for _, mb := range enh.signatureToMetricWithBuckets { if len(mb.buckets) \u0026gt; 0 { enh.Out = append(enh.Out, Sample{ Metric: mb.metric, F: bucketQuantile(q, mb.buckets), }) } } return enh.Out } // bucketQuantile calculates the quantile \u0026#39;q\u0026#39; based on the given buckets. The // buckets will be sorted by upperBound by this function (i.e. no sorting // needed before calling this function). The quantile value is interpolated // assuming a linear distribution within a bucket. However, if the quantile // falls into the highest bucket, the upper bound of the 2nd highest bucket is // returned. A natural lower bound of 0 is assumed if the upper bound of the // lowest bucket is greater 0. In that case, interpolation in the lowest bucket // happens linearly between 0 and the upper bound of the lowest bucket. // However, if the lowest bucket has an upper bound less or equal 0, this upper // bound is returned if the quantile falls into the lowest bucket. // // There are a number of special cases (once we have a way to report errors // happening during evaluations of AST functions, we should report those // explicitly): // // If \u0026#39;buckets\u0026#39; has 0 observations, NaN is returned. // // If \u0026#39;buckets\u0026#39; has fewer than 2 elements, NaN is returned. // // If the highest bucket is not +Inf, NaN is returned. // // If q==NaN, NaN is returned. // // If q\u0026lt;0, -Inf is returned. // // If q\u0026gt;1, +Inf is returned. func bucketQuantile(q float64, buckets buckets) float64 { if math.IsNaN(q) { return math.NaN() } if q \u0026lt; 0 { return math.Inf(-1) } if q \u0026gt; 1 { return math.Inf(+1) } slices.SortFunc(buckets, func(a, b bucket) bool { return a.upperBound \u0026lt; b.upperBound }) if !math.IsInf(buckets[len(buckets)-1].upperBound, +1) { return math.NaN() } buckets = coalesceBuckets(buckets) ensureMonotonic(buckets) if len(buckets) \u0026lt; 2 { return math.NaN() } observations := buckets[len(buckets)-1].count if observations == 0 { return math.NaN() } rank := q * observations b := sort.Search(len(buckets)-1, func(i int) bool { return buckets[i].count \u0026gt;= rank }) if b == len(buckets)-1 { return buckets[len(buckets)-2].upperBound } if b == 0 \u0026amp;\u0026amp; buckets[0].upperBound \u0026lt;= 0 { return buckets[0].upperBound } var ( bucketStart float64 bucketEnd = buckets[b].upperBound count = buckets[b].count ) if b \u0026gt; 0 { bucketStart = buckets[b-1].upperBound count -= buckets[b-1].count rank -= buckets[b-1].count } return bucketStart + (bucketEnd-bucketStart)*(rank/count) } 相关资料\nhttps://stackoverflow.com/questions/55162093/understanding-histogram-quantile-based-on-rate-in-prometheus https://stackoverflow.com/questions/60962520/how-to-get-the-quantile-of-rate-in-prometheus/65418483#65418483 可以在 prometheus 项目下执行下面的单测：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func TestBucketQuantile__rate5m(t *testing.T) { q := 0.4 myBuckets := buckets{ {10, 5}, {20, 8}, {30, 12}, {40, 15}, {math.Inf(1), 15}, } f := bucketQuantile(q, myBuckets) println(\u0026#34;==== before rate =====\u0026#34;) fmt.Println(myBuckets) fmt.Printf(\u0026#34;%.2f\\n\u0026#34;, f) for i, myBucket := range myBuckets { myBuckets[i].count = myBucket.count / float64(5*60) } println(\u0026#34;==== after rate =====\u0026#34;) fmt.Println(myBuckets) f1 := bucketQuantile(q, myBuckets) fmt.Printf(\u0026#34;%.2f\\n\u0026#34;, f1) } ","date":"2022-10-11T22:09:19+08:00","permalink":"https://zhumengzhu.github.io/2022/10/prometheus-and-grafana-learning-notes/","title":"Prometheus \u0026 Grafana 学习记录"},{"content":"资料：\nR 树基础：空间数据索引RTree（R树）完全解析及Java实现 实用场景 基于HBase与静态多级格网索引的地表覆盖数据高效检索方法 如何理解 R 树 R 树是平衡树； 平衡树要求节点有序； R 树的节点是高维数据，比如对于二维来说，存储的是 MBR(最小限定矩形)； 高维数据的顺序如何定义？ 对一维的线段来说，定义一个坐标系，存在一个原点，线段左端点距原点的距离作为排序的依据； 对二维的MBR来说(可以用左下，右上两个点的坐标表示一个矩形)，怎么定义？ 猜想，定义平面直角坐标系，以左下点的坐标(x,y)作为排序的依据：比如(1,2)\u0026lt;(1,3)\u0026lt;(2,1)； 基本概念 MBR Packed 1-D R-Tree 2-D R-Tree 上图中，总计 12 个实线框，正好对应 R 树中 12 个叶子节点 R8~R19;\n实际应用 道路索引 每一条路使用一个最小MBB来进行包裹，使它成为R树的叶子结点（也就是那些数据结点）\n将每一条路的最小MBB作为R树的数据单元来进行构建R树(这里采用的是R树的改进版本R*树)\n","date":"2022-09-28T21:52:53+08:00","permalink":"https://zhumengzhu.github.io/2022/09/r-tree-and-its-family/","title":"R树及其家族"},{"content":"背景 为了向用户提供天气服务，计划接入三方天气 API，但有些三方的 API 按调用次数收费；为了节省成本，考虑使用缓存以减少调用次数。具体如何实现呢？\n考虑到用户的请求参数经度和纬度两个参数，直接使用经纬度作为缓存的 key 显然不合适，因为经纬度是连续的、无限的，因此需要使用空间索引技术。\n首先想到的自然是 GeoHash，它是空间索引的一种实现，将地球上的二维经纬度映射到一维的整数空间，然后使用整数空间进行索引。\nGeoHash 和 Redis 资料一 https://medium.com/groupon-eng/scaling-millions-of-geospatial-queries-per-minute-using-redis-7c05bcf6b4db\nGEOADD command can be used to index spatial entities in Redis.\nThe time complexity for this command is O(log(N)) for each item added, where N is the number of elements in the sorted set.\nTo perform a spatial search of entities GEORADIUS or GEORADIUSBYMEMBER can be used in Redis 3.\u0026gt; 2.0 (and above) and GEOSEARCH or GEOSEARCHSTORE can be used in Redis 6.2 (and above).\nThe time complexity for these commands is approximately O(N+log(M)) where N is the number of \u0026gt; elements inside the bounding box of the circular area delimited by center and radius, and M is \u0026gt; the number of items inside the index.\nThere are plenty of solutions available for implementing spatial searches.\nData structures like Quadtree, R-tree, and K-d tree can be used to index the entities.\nGeospatial indexers like S2 and H3 can be used for similar queries.\n疑问：\n为什么 GEOADD 复杂度是 O(log(N))，为什么 ZADD 复杂度是 O(log(N))，两者有什么关系？ 为什么 GEORADIUS 复杂度是 O(N + log(M)), 为什么 ZRANGE 复杂度是 O(N+log(M))，两者有什么关系？ 解答：\nZADD 与 ZRANGE 的时间复杂度：Redis中Sorted-Set时间复杂度和实战 GEOADD 底层基于 ZADD，所以复杂度相同； GEORADIUS 底层基于 ZRANGE，但是更复杂，所以复杂度相同，但由于会搜索周边 9 个方格，因此系数更大； 资料二 https://www.memurai.com/blog/geospatial-queries-in-redis\nRedis 不存储原始的坐标\nGEOADD cities -122.34 47.61 Seattle ZRANGE cities 0 -1 WITHSCORES GEOHASH cities Seattle GEOPOS cities Seattle\nNotice that we provided –122.34 as longitude and 47.61 as latitude when calling GEOADD, but \u0026gt; received –122.33999758958816528 and 47.61000085169597185, respectively, in return.\nThis is an unfortunate consequence of Redis using a 52-bit geohash to store the coordinates:\nRedis does not store the raw set of coordinates that we provide, but instead decodes the internally stored geohash to generate a GEOPOS response.\n资料三 https://luoming1224.github.io/2019/04/08/%5Bredis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%5Dredis%E4%B8%ADGeo%E5%91%BD%E4%BB%A4%E4%BB%8B%E7%BB%8D/ https://luoming1224.github.io/2019/04/04/[redis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0]GeoHash%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3%E5%8F%8A%E5%85%B6%E5%AE%9E%E7%8E%B0/ https://developer.aliyun.com/article/62844#slide-16 https://blog.huangz.me/diary/2015/annotated-redis-geo-source.html GeoSearch 原理 geohash编码越长精度越高，相反编码越短，表示的范围越大，前缀相同的字符串表示的范围接近，根据中心点位置和搜索范围\u0026gt; 距离计算geohash编码的长度和geohash编码，然后搜索以该geohash编码为前缀的点以及周边８个范围内的点，返回满足要求\u0026gt; 的点。\n函数geohashGetAreasByRadiusWGS84根据中心点位置和搜索范围距离计算geohash编码的长度和geohash编码，以及在\u0026gt; geohash长度编码的基础上，计算周边8个区块的geohash。\n函数membersOfAllNeighbors对中心点以及它周边八个方向进行查找，找出所有范围内的元素，返回满足搜索距离范围的点。\u0026gt; 该函数中依次对中心点及周边8个区块调用membersOfGeoHashBox函数。\nlong_range和lat_range为地球经纬度的范围；hash-\u0026gt;bits用户保存最终二进制编码结果，hash-\u0026gt;step是经纬度划分的次\u0026gt; 数，在redis中该值为26，即经度/纬度的二进制编码长度为26，最终经交叉组合而成的地理位置的二进制编码为52位。Base32\u0026gt; 编码为每5bits组成一个字符，所以最终的GeoHash字符串为11位。\n为什么是52位？因为在redis中是把地理位置编码后的二进制值存入zset数据结构中，double类型的尾数部分长度为52位。\nRedis 的设计思路推演 GeoHash 除了可以对空间进行编码之外，还有两个关键特性，使得 Redis 可以通过组合 GeoHash 与 ZSET，实现空间索引功能，这两个特性是：\n哈希可以有任意精度；\n相近点有相似的前缀，这使得可以通过比较 geohash 值查找附近的点； 思路推演：\n当只使用 GeoHash 时，我们可以很方便的判定两个点是否在属于同一个区域：\nGeoHash(point1, precision=7) == GeoHash(point2, precision=7) 如果将 hash 值相同的点放入 set，那么就可以实现“周边搜”：\n首先计算要搜索点都 geohash 值； 然后以该值作为 key 去查找对应的 set； 上述方法，需要预先指定 geohash 的精度，存储和查询时都必须以相同的精度进行，假如：\n存储时：sadd(GeoHash(point1, precision=7), point1); 读取时：执行 smembers(GeoHash(point1, precision=6)), 由于精度不同，geohash 值不同，此时无法将 point1 查找出来; 假如用上述方法实现地图搜索功能，意味着，如果以 3KM 的网格存储，那么查询也必须以 3KM 进行查，这样的限制太大；\n此时可以利用“相近点有相似前缀”特性，对 geohash 进行排序，也就是用 geohash 作为索引：\n此时，如果存储时精度为 7，那么搜索可以支持大于等 7 的任意精度；\n在 Redis 中，支持排序的显然是 ZSET，因此，Redis 最终在 3.2 版本选择了组合 GeoHash 和 ZSET 实现了相关的空间索引功能； 看起来很完美，但是，GeoHash 只是对大部分点来说，编码相似距离也相近，实际它存在突变性，也就是编码相邻但距离可能很远：\nGeoHash 存在缺陷：https://www.jianshu.com/p/1ecf03293b9a\n疑问：\n怎么找周围 8 个区域的 GeoHash？ GeoHash 的缺陷及解决 https://patents.google.com/patent/CN103383682A/zh https://www.developers.pub/article/637 Google S2/Uber h3 https://juejin.cn/post/6921977063477870606 https://github.com/halfrost/Halfrost-Field/blob/master/contents/Go/go_s2_CellID.md https://www.jianshu.com/p/3dbaf73a09af https://blog.csdn.net/qq_43777978/article/details/116800460 https://blog.csdn.net/alinyua/article/details/105803546 为什么能够做网格空间索引的形状只有三角形、矩形和六边形(https://www.biaodianfu.com/uber-h3.html): H3是一种基于网格的空间索引，但跟普通的矩形网格索引不同的是，他的每一个网格都是正六边形。为啥要选正六边形呢，因为在基于网格的空间索引中，使用的多边形的边数越多，则一个网格越近似圆形，做缓冲区查询、kNN查询什么的也就越方便。而做网格索引又要求空间能够被网格铺满，不能有缝隙。根据初中数学知识，我们知道一个多边形的内角和公式为：\n𝜃=(𝑥–2)∗180∘\n其中，x为多边形的边数，𝜃为多边形的内角和。则一个正多边形的每个角的角度为 𝜃𝑥=(𝑥–2)∗180∘𝑥，而如果需要多边形能够铺满空间，则在多边形的顶点相交处，设每个顶点有y个多边形相交，需要满足以下等式：\n360∘𝑦=(𝑥–2)∗180∘𝑥\n以上等式的求解过程我不再赘述，这个等式只有三组整数解：x=3,y=6；x=4,y=4；x=6,y=3。\n因此，能够做网格空间索引的形状只有三角形、矩形和六边形，而六边形因为边数最多，最接近圆，所以理论上来说在某些场景下是最优的选择。\n其它 什么是WGS84坐标系 https://zhuanlan.zhihu.com/p/97363931\n","date":"2022-09-22T21:31:50+08:00","permalink":"https://zhumengzhu.github.io/2022/09/spatial-index-technology-first-glimpse/","title":"空间索引技术初见"}]